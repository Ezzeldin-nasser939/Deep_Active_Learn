{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ezzeldin-nasser939/Deep_Active_Learn/blob/main/Deep_Active_Learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba4LRIE4A0rb"
      },
      "source": [
        "# ***Imports***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJXBUWvcUsZ0",
        "outputId": "40510d66-3e9a-4f0f-e73c-791a9828cb02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deep-active-learning'...\n",
            "remote: Enumerating objects: 2945, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 2945 (delta 36), reused 42 (delta 23), pack-reused 2870\u001b[K\n",
            "Receiving objects: 100% (2945/2945), 1.60 GiB | 28.60 MiB/s, done.\n",
            "Resolving deltas: 100% (1159/1159), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cure-lab/deep-active-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAlLulqhU_Dk",
        "outputId": "e2c7d3f9-3987-4e03-f7e4-d374f5fb57ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting batchbald_redux==2.0.5\n",
            "  Downloading batchbald_redux-2.0.5-py3-none-any.whl (16 kB)\n",
            "Collecting gurobipy==9.5.1\n",
            "  Downloading gurobipy-9.5.1-cp310-cp310-manylinux2014_x86_64.whl (11.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib==1.1.0\n",
            "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.0/307.0 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib==3.1.3\n",
            "  Downloading matplotlib-3.1.3.tar.gz (40.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.21\n",
            "  Downloading numpy-1.21.0.zip (10.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pandas==1.0.3\n",
            "  Downloading pandas-1.0.3.tar.gz (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -r /content/deep-active-learning/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QD5PW4oXktaT",
        "outputId": "62b0b95b-c531-4c81-efba-8f80a1f17538"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting batchbald_redux\n",
            "  Using cached batchbald_redux-2.0.5-py3-none-any.whl (16 kB)\n",
            "Collecting blackhc.project~=2.1\n",
            "  Downloading blackhc.project-2.6.0-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from batchbald_redux) (4.65.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from batchbald_redux) (0.15.1+cu118)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from batchbald_redux) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from batchbald_redux) (1.22.4)\n",
            "Collecting toma<2\n",
            "  Downloading toma-1.1.0-py3-none-any.whl (9.6 kB)\n",
            "Collecting fs.sshfs\n",
            "  Downloading fs.sshfs-1.0.1-py2.py3-none-any.whl (24 kB)\n",
            "Collecting laaos\n",
            "  Downloading laaos-2.1.1-py3-none-any.whl (7.8 kB)\n",
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from toma<2->batchbald_redux) (5.9.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->batchbald_redux) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->batchbald_redux) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->batchbald_redux) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->batchbald_redux) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->batchbald_redux) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->batchbald_redux) (3.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->batchbald_redux) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->batchbald_redux) (16.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->batchbald_redux) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->batchbald_redux) (2.27.1)\n",
            "Collecting fs~=2.2\n",
            "  Downloading fs-2.4.16-py2.py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six~=1.10 in /usr/local/lib/python3.10/dist-packages (from fs.sshfs->blackhc.project~=2.1->batchbald_redux) (1.16.0)\n",
            "Collecting paramiko~=2.0\n",
            "  Downloading paramiko-2.12.0-py2.py3-none-any.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.1/213.1 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.10/dist-packages (from fs.sshfs->blackhc.project~=2.1->batchbald_redux) (67.7.2)\n",
            "Collecting property-cached~=1.6\n",
            "  Downloading property_cached-1.6.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->batchbald_redux) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->batchbald_redux) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->batchbald_redux) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->batchbald_redux) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->batchbald_redux) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->batchbald_redux) (1.3.0)\n",
            "Requirement already satisfied: appdirs~=1.4.3 in /usr/local/lib/python3.10/dist-packages (from fs~=2.2->fs.sshfs->blackhc.project~=2.1->batchbald_redux) (1.4.4)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.10/dist-packages (from paramiko~=2.0->fs.sshfs->blackhc.project~=2.1->batchbald_redux) (40.0.2)\n",
            "Collecting pynacl>=1.0.1\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bcrypt>=3.1.3\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.5->paramiko~=2.0->fs.sshfs->blackhc.project~=2.1->batchbald_redux) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.5->paramiko~=2.0->fs.sshfs->blackhc.project~=2.1->batchbald_redux) (2.21)\n",
            "Installing collected packages: laaos, smmap, property-cached, fs, bcrypt, pynacl, gitdb, paramiko, gitpython, fs.sshfs, blackhc.project, toma, batchbald_redux\n",
            "Successfully installed batchbald_redux-2.0.5 bcrypt-4.0.1 blackhc.project-2.6.0 fs-2.4.16 fs.sshfs-1.0.1 gitdb-4.0.10 gitpython-3.1.31 laaos-2.1.1 paramiko-2.12.0 property-cached-1.6.4 pynacl-1.5.0 smmap-5.0.0 toma-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install batchbald_redux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fmI_8DrCkjVr",
        "outputId": "5c6e1d3f-4695-4c8e-bae0-dc5845d4e054"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchfile\n",
            "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torchfile\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5709 sha256=266622a1d373d77bfbd222b7861ad5e9c5ed27fa1dfcc9ce9c6e41dd1db72865\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/e9/87/1c51daf8e468d5c14931f8ac3344880f903ba96b063675cac2\n",
            "Successfully built torchfile\n",
            "Installing collected packages: torchfile\n",
            "Successfully installed torchfile-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gefmbMUL3-6G"
      },
      "source": [
        "# ***`Stratgies`***\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X1xU2TI4gi6"
      },
      "source": [
        "# ***Cifar10 Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXae9WB2Xe4G"
      },
      "source": [
        "main.py: error: argument --strategy: invalid choice: '/content/deep-active-learning/query_strategies/margin_sampling.py' (choose from 'ActiveLearningByLearning', 'AdversarialBIM', 'AdversarialDeepFool', 'BALDDropout', 'BadgeSampling', 'BaselineSampling', 'BatchBALD', 'ClusterMarginSampling', 'CoreSet', 'EntropySampling', 'EntropySamplingDropout', 'KCenterGreedy', 'KMeansSampling', 'LearningLoss', 'LeastConfidence', 'LeastConfidenceDropout', 'MCADL', 'MarginSampling', 'MarginSamplingDropout', 'RandomSampling', 'VAAL', 'WAAL', 'coreGCN', 'fixmatch', 'flexmatch', 'pseudolabel', 'ssl_Consistency', 'ssl_Diff2AugDirect', 'ssl_Diff2AugKmeans', 'ssl_LC', 'ssl_Random', 'uda', 'uncertainGCN')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyCleWUQVszB",
        "outputId": "73fa2a2c-40c9-4080-bd51-36cde29375c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "save path : ./save\n",
            "{'strategy': 'EntropySampling', 'nQuery': 1, 'nStart': 20.0, 'nEnd': 20.0, 'nEmb': 256, 'seed': 1, 'model': 'ResNet18', 'dataset': 'cifar10', 'data_path': './datasets', 'save_path': './save', 'save_file': 'result.csv', 'hidden_units': 128, 'dropout_rate': 0.3, 'lambda_loss': 1.2, 's_margin': 0.1, 'n_ensembles': 1, 'proxy_model': None, 'optimizer': 'SGD', 'n_epoch': 30, 'schedule': [80, 120], 'momentum': 0.9, 'lr': 0.1, 'gammas': [0.1, 0.1], 'save_model': False, 'load_ckpt': False, 'add_imagenet': False}\n",
            "Random Seed: 1\n",
            "python version : 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]\n",
            "torch  version : 2.0.0+cu118\n",
            "cudnn  version : 8700\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./datasets/cifar10/cifar-10-python.tar.gz\n",
            "100% 170498071/170498071 [00:05<00:00, 29245312.52it/s]\n",
            "Extracting ./datasets/cifar10/cifar-10-python.tar.gz to ./datasets/cifar10\n",
            "Files already downloaded and verified\n",
            "[init=10000] [query=500] [end=10000]\n",
            "Strategy EntropySampling successfully loaded...\n",
            "Let's use 1 GPUs!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "[Batch=000] [Loss=2.36]\n",
            "[Batch=010] [Loss=2.19]\n",
            "[Batch=020] [Loss=1.84]\n",
            "[Batch=030] [Loss=1.96]\n",
            "\n",
            "==>>[2023-04-30 15:46:07] [Epoch=000/030] [EntropySampling Need: 00:00:00] [LR=0.1000] [Best : Test Accuracy=0.00, Error=1.00]\n",
            "[Batch=000] [Loss=2.00]\n",
            "[Batch=010] [Loss=1.68]\n",
            "[Batch=020] [Loss=1.70]\n",
            "[Batch=030] [Loss=1.66]\n",
            "\n",
            "==>>[2023-04-30 15:46:16] [Epoch=001/030] [EntropySampling Need: 00:07:30] [LR=0.1000] [Best : Test Accuracy=0.29, Error=0.71]\n",
            "[Batch=000] [Loss=1.70]\n",
            "[Batch=010] [Loss=1.83]\n",
            "[Batch=020] [Loss=1.61]\n",
            "[Batch=030] [Loss=1.58]\n",
            "\n",
            "==>>[2023-04-30 15:46:27] [Epoch=002/030] [EntropySampling Need: 00:05:50] [LR=0.1000] [Best : Test Accuracy=0.34, Error=0.66]\n",
            "[Batch=000] [Loss=1.49]\n",
            "[Batch=010] [Loss=1.49]\n",
            "[Batch=020] [Loss=1.48]\n",
            "[Batch=030] [Loss=1.49]\n",
            "\n",
            "==>>[2023-04-30 15:46:36] [Epoch=003/030] [EntropySampling Need: 00:05:20] [LR=0.1000] [Best : Test Accuracy=0.44, Error=0.56]\n",
            "[Batch=000] [Loss=1.57]\n",
            "[Batch=010] [Loss=1.49]\n",
            "[Batch=020] [Loss=1.47]\n",
            "[Batch=030] [Loss=1.37]\n",
            "\n",
            "==>>[2023-04-30 15:46:46] [Epoch=004/030] [EntropySampling Need: 00:04:49] [LR=0.1000] [Best : Test Accuracy=0.44, Error=0.56]\n",
            "[Batch=000] [Loss=1.46]\n",
            "[Batch=010] [Loss=1.54]\n",
            "[Batch=020] [Loss=1.40]\n",
            "[Batch=030] [Loss=1.28]\n",
            "\n",
            "==>>[2023-04-30 15:46:55] [Epoch=005/030] [EntropySampling Need: 00:04:35] [LR=0.1000] [Best : Test Accuracy=0.44, Error=0.56]\n",
            "[Batch=000] [Loss=1.35]\n",
            "[Batch=010] [Loss=1.36]\n",
            "[Batch=020] [Loss=1.37]\n",
            "[Batch=030] [Loss=1.29]\n",
            "\n",
            "==>>[2023-04-30 15:47:03] [Epoch=006/030] [EntropySampling Need: 00:04:17] [LR=0.1000] [Best : Test Accuracy=0.44, Error=0.56]\n",
            "[Batch=000] [Loss=1.32]\n",
            "[Batch=010] [Loss=1.31]\n",
            "[Batch=020] [Loss=1.24]\n",
            "[Batch=030] [Loss=1.20]\n",
            "\n",
            "==>>[2023-04-30 15:47:13] [Epoch=007/030] [EntropySampling Need: 00:03:57] [LR=0.1000] [Best : Test Accuracy=0.48, Error=0.52]\n",
            "[Batch=000] [Loss=1.32]\n",
            "[Batch=010] [Loss=1.26]\n",
            "[Batch=020] [Loss=1.10]\n",
            "[Batch=030] [Loss=1.17]\n",
            "\n",
            "==>>[2023-04-30 15:47:22] [Epoch=008/030] [EntropySampling Need: 00:03:45] [LR=0.1000] [Best : Test Accuracy=0.52, Error=0.48]\n",
            "[Batch=000] [Loss=1.08]\n",
            "[Batch=010] [Loss=1.11]\n",
            "[Batch=020] [Loss=1.07]\n",
            "[Batch=030] [Loss=1.25]\n",
            "\n",
            "==>>[2023-04-30 15:47:30] [Epoch=009/030] [EntropySampling Need: 00:03:32] [LR=0.1000] [Best : Test Accuracy=0.57, Error=0.43]\n",
            "[Batch=000] [Loss=1.07]\n",
            "[Batch=010] [Loss=1.12]\n",
            "[Batch=020] [Loss=1.23]\n",
            "[Batch=030] [Loss=1.03]\n",
            "\n",
            "==>>[2023-04-30 15:47:40] [Epoch=010/030] [EntropySampling Need: 00:03:18] [LR=0.1000] [Best : Test Accuracy=0.58, Error=0.42]\n",
            "[Batch=000] [Loss=0.96]\n",
            "[Batch=010] [Loss=0.93]\n",
            "[Batch=020] [Loss=1.06]\n",
            "[Batch=030] [Loss=1.01]\n",
            "\n",
            "==>>[2023-04-30 15:47:50] [Epoch=011/030] [EntropySampling Need: 00:03:08] [LR=0.1000] [Best : Test Accuracy=0.58, Error=0.42]\n",
            "[Batch=000] [Loss=1.07]\n",
            "[Batch=010] [Loss=1.05]\n",
            "[Batch=020] [Loss=1.05]\n",
            "[Batch=030] [Loss=1.04]\n",
            "\n",
            "==>>[2023-04-30 15:47:58] [Epoch=012/030] [EntropySampling Need: 00:02:57] [LR=0.1000] [Best : Test Accuracy=0.58, Error=0.42]\n",
            "[Batch=000] [Loss=0.99]\n",
            "[Batch=010] [Loss=0.94]\n",
            "[Batch=020] [Loss=0.94]\n",
            "[Batch=030] [Loss=0.98]\n",
            "\n",
            "==>>[2023-04-30 15:48:07] [Epoch=013/030] [EntropySampling Need: 00:02:45] [LR=0.1000] [Best : Test Accuracy=0.59, Error=0.41]\n",
            "[Batch=000] [Loss=0.91]\n",
            "[Batch=010] [Loss=0.96]\n",
            "[Batch=020] [Loss=0.90]\n",
            "[Batch=030] [Loss=0.82]\n",
            "\n",
            "==>>[2023-04-30 15:48:17] [Epoch=014/030] [EntropySampling Need: 00:02:35] [LR=0.1000] [Best : Test Accuracy=0.59, Error=0.41]\n",
            "[Batch=000] [Loss=0.95]\n",
            "[Batch=010] [Loss=1.05]\n",
            "[Batch=020] [Loss=1.02]\n",
            "[Batch=030] [Loss=0.86]\n",
            "\n",
            "==>>[2023-04-30 15:48:25] [Epoch=015/030] [EntropySampling Need: 00:02:25] [LR=0.1000] [Best : Test Accuracy=0.59, Error=0.41]\n",
            "[Batch=000] [Loss=0.86]\n",
            "[Batch=010] [Loss=0.93]\n",
            "[Batch=020] [Loss=0.85]\n",
            "[Batch=030] [Loss=0.89]\n",
            "\n",
            "==>>[2023-04-30 15:48:35] [Epoch=016/030] [EntropySampling Need: 00:02:15] [LR=0.1000] [Best : Test Accuracy=0.62, Error=0.38]\n",
            "[Batch=000] [Loss=0.85]\n",
            "[Batch=010] [Loss=0.76]\n",
            "[Batch=020] [Loss=0.81]\n",
            "[Batch=030] [Loss=0.90]\n",
            "\n",
            "==>>[2023-04-30 15:48:45] [Epoch=017/030] [EntropySampling Need: 00:02:05] [LR=0.1000] [Best : Test Accuracy=0.63, Error=0.37]\n",
            "[Batch=000] [Loss=0.94]\n",
            "[Batch=010] [Loss=0.95]\n",
            "[Batch=020] [Loss=0.76]\n",
            "[Batch=030] [Loss=0.85]\n",
            "\n",
            "==>>[2023-04-30 15:48:53] [Epoch=018/030] [EntropySampling Need: 00:01:55] [LR=0.1000] [Best : Test Accuracy=0.65, Error=0.35]\n",
            "[Batch=000] [Loss=0.79]\n",
            "[Batch=010] [Loss=0.99]\n",
            "[Batch=020] [Loss=0.87]\n",
            "[Batch=030] [Loss=0.80]\n",
            "\n",
            "==>>[2023-04-30 15:49:03] [Epoch=019/030] [EntropySampling Need: 00:01:45] [LR=0.1000] [Best : Test Accuracy=0.66, Error=0.34]\n",
            "[Batch=000] [Loss=0.74]\n",
            "[Batch=010] [Loss=0.89]\n",
            "[Batch=020] [Loss=0.78]\n",
            "[Batch=030] [Loss=0.72]\n",
            "\n",
            "==>>[2023-04-30 15:49:12] [Epoch=020/030] [EntropySampling Need: 00:01:35] [LR=0.1000] [Best : Test Accuracy=0.66, Error=0.34]\n",
            "[Batch=000] [Loss=0.71]\n",
            "[Batch=010] [Loss=0.94]\n",
            "[Batch=020] [Loss=0.83]\n",
            "[Batch=030] [Loss=0.72]\n",
            "\n",
            "==>>[2023-04-30 15:49:20] [Epoch=021/030] [EntropySampling Need: 00:01:26] [LR=0.1000] [Best : Test Accuracy=0.67, Error=0.33]\n",
            "[Batch=000] [Loss=0.72]\n",
            "[Batch=010] [Loss=0.80]\n",
            "[Batch=020] [Loss=0.69]\n",
            "[Batch=030] [Loss=0.88]\n",
            "\n",
            "==>>[2023-04-30 15:49:30] [Epoch=022/030] [EntropySampling Need: 00:01:16] [LR=0.1000] [Best : Test Accuracy=0.67, Error=0.33]\n",
            "[Batch=000] [Loss=0.72]\n",
            "[Batch=010] [Loss=0.74]\n",
            "[Batch=020] [Loss=0.72]\n",
            "[Batch=030] [Loss=0.71]\n",
            "\n",
            "==>>[2023-04-30 15:49:38] [Epoch=023/030] [EntropySampling Need: 00:01:06] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.64]\n",
            "[Batch=010] [Loss=0.70]\n",
            "[Batch=020] [Loss=0.96]\n",
            "[Batch=030] [Loss=0.68]\n",
            "\n",
            "==>>[2023-04-30 15:49:47] [Epoch=024/030] [EntropySampling Need: 00:00:56] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.77]\n",
            "[Batch=010] [Loss=0.87]\n",
            "[Batch=020] [Loss=0.85]\n",
            "[Batch=030] [Loss=0.67]\n",
            "\n",
            "==>>[2023-04-30 15:49:57] [Epoch=025/030] [EntropySampling Need: 00:00:47] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.74]\n",
            "[Batch=010] [Loss=0.69]\n",
            "[Batch=020] [Loss=0.65]\n",
            "[Batch=030] [Loss=0.65]\n",
            "\n",
            "==>>[2023-04-30 15:50:05] [Epoch=026/030] [EntropySampling Need: 00:00:37] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.72]\n",
            "[Batch=010] [Loss=0.63]\n",
            "[Batch=020] [Loss=0.62]\n",
            "[Batch=030] [Loss=0.58]\n",
            "\n",
            "==>>[2023-04-30 15:50:14] [Epoch=027/030] [EntropySampling Need: 00:00:28] [LR=0.1000] [Best : Test Accuracy=0.71, Error=0.29]\n",
            "[Batch=000] [Loss=0.66]\n",
            "[Batch=010] [Loss=0.78]\n",
            "[Batch=020] [Loss=0.74]\n",
            "[Batch=030] [Loss=0.70]\n",
            "\n",
            "==>>[2023-04-30 15:50:24] [Epoch=028/030] [EntropySampling Need: 00:00:18] [LR=0.1000] [Best : Test Accuracy=0.71, Error=0.29]\n",
            "[Batch=000] [Loss=0.59]\n",
            "[Batch=010] [Loss=0.54]\n",
            "[Batch=020] [Loss=0.59]\n",
            "[Batch=030] [Loss=0.59]\n",
            "\n",
            "==>>[2023-04-30 15:50:32] [Epoch=029/030] [EntropySampling Need: 00:00:09] [LR=0.1000] [Best : Test Accuracy=0.71, Error=0.29]\n",
            "---- save figure the accuracy/loss curve of train/val into ./save/cifar10\n",
            "==>> Testing accuracy 0.7231\n",
            "success!\n"
          ]
        }
      ],
      "source": [
        "!python /content/deep-active-learning/main.py --model ResNet18  --nStart 20  --dataset cifar10 --strategy EntropySampling --nEnd=20 --n_epoch=30 --seed 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDQO24qS3Zom",
        "outputId": "483ae533-ce1a-4861-b0b1-ad667efb41f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "save path : ./save\n",
            "{'strategy': 'KMeansSampling', 'nQuery': 1, 'nStart': 20.0, 'nEnd': 20.0, 'nEmb': 256, 'seed': 1, 'model': 'ResNet18', 'dataset': 'cifar10', 'data_path': './datasets', 'save_path': './save', 'save_file': 'result.csv', 'hidden_units': 128, 'dropout_rate': 0.3, 'lambda_loss': 1.2, 's_margin': 0.1, 'n_ensembles': 1, 'proxy_model': None, 'optimizer': 'SGD', 'n_epoch': 30, 'schedule': [80, 120], 'momentum': 0.9, 'lr': 0.1, 'gammas': [0.1, 0.1], 'save_model': False, 'load_ckpt': False, 'add_imagenet': False}\n",
            "Random Seed: 1\n",
            "python version : 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]\n",
            "torch  version : 2.0.0+cu118\n",
            "cudnn  version : 8700\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[init=10000] [query=500] [end=10000]\n",
            "Strategy KMeansSampling successfully loaded...\n",
            "Let's use 1 GPUs!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "[Batch=000] [Loss=2.36]\n",
            "[Batch=010] [Loss=2.10]\n",
            "[Batch=020] [Loss=1.91]\n",
            "[Batch=030] [Loss=1.92]\n",
            "\n",
            "==>>[2023-04-30 15:53:28] [Epoch=000/030] [KMeansSampling Need: 00:00:00] [LR=0.1000] [Best : Test Accuracy=0.00, Error=1.00]\n",
            "[Batch=000] [Loss=1.94]\n",
            "[Batch=010] [Loss=1.73]\n",
            "[Batch=020] [Loss=1.69]\n",
            "[Batch=030] [Loss=1.64]\n",
            "\n",
            "==>>[2023-04-30 15:53:37] [Epoch=001/030] [KMeansSampling Need: 00:05:14] [LR=0.1000] [Best : Test Accuracy=0.26, Error=0.74]\n",
            "[Batch=000] [Loss=1.68]\n",
            "[Batch=010] [Loss=1.66]\n",
            "[Batch=020] [Loss=1.66]\n",
            "[Batch=030] [Loss=1.58]\n",
            "\n",
            "==>>[2023-04-30 15:53:46] [Epoch=002/030] [KMeansSampling Need: 00:04:43] [LR=0.1000] [Best : Test Accuracy=0.26, Error=0.74]\n",
            "[Batch=000] [Loss=1.64]\n",
            "[Batch=010] [Loss=1.51]\n",
            "[Batch=020] [Loss=1.51]\n",
            "[Batch=030] [Loss=1.53]\n",
            "\n",
            "==>>[2023-04-30 15:53:55] [Epoch=003/030] [KMeansSampling Need: 00:04:23] [LR=0.1000] [Best : Test Accuracy=0.40, Error=0.60]\n",
            "[Batch=000] [Loss=1.52]\n",
            "[Batch=010] [Loss=1.50]\n",
            "[Batch=020] [Loss=1.48]\n",
            "[Batch=030] [Loss=1.38]\n",
            "\n",
            "==>>[2023-04-30 15:54:05] [Epoch=004/030] [KMeansSampling Need: 00:04:07] [LR=0.1000] [Best : Test Accuracy=0.44, Error=0.56]\n",
            "[Batch=000] [Loss=1.52]\n",
            "[Batch=010] [Loss=1.47]\n",
            "[Batch=020] [Loss=1.46]\n",
            "[Batch=030] [Loss=1.30]\n",
            "\n",
            "==>>[2023-04-30 15:54:14] [Epoch=005/030] [KMeansSampling Need: 00:03:58] [LR=0.1000] [Best : Test Accuracy=0.44, Error=0.56]\n",
            "[Batch=000] [Loss=1.32]\n",
            "[Batch=010] [Loss=1.36]\n",
            "[Batch=020] [Loss=1.37]\n",
            "[Batch=030] [Loss=1.28]\n",
            "\n",
            "==>>[2023-04-30 15:54:23] [Epoch=006/030] [KMeansSampling Need: 00:03:46] [LR=0.1000] [Best : Test Accuracy=0.48, Error=0.52]\n",
            "[Batch=000] [Loss=1.30]\n",
            "[Batch=010] [Loss=1.32]\n",
            "[Batch=020] [Loss=1.17]\n",
            "[Batch=030] [Loss=1.20]\n",
            "\n",
            "==>>[2023-04-30 15:54:32] [Epoch=007/030] [KMeansSampling Need: 00:03:35] [LR=0.1000] [Best : Test Accuracy=0.50, Error=0.50]\n",
            "[Batch=000] [Loss=1.25]\n",
            "[Batch=010] [Loss=1.33]\n",
            "[Batch=020] [Loss=1.19]\n",
            "[Batch=030] [Loss=1.12]\n",
            "\n",
            "==>>[2023-04-30 15:54:41] [Epoch=008/030] [KMeansSampling Need: 00:03:26] [LR=0.1000] [Best : Test Accuracy=0.50, Error=0.50]\n",
            "[Batch=000] [Loss=1.09]\n",
            "[Batch=010] [Loss=1.14]\n",
            "[Batch=020] [Loss=1.09]\n",
            "[Batch=030] [Loss=1.24]\n",
            "\n",
            "==>>[2023-04-30 15:54:50] [Epoch=009/030] [KMeansSampling Need: 00:03:16] [LR=0.1000] [Best : Test Accuracy=0.53, Error=0.47]\n",
            "[Batch=000] [Loss=1.11]\n",
            "[Batch=010] [Loss=1.11]\n",
            "[Batch=020] [Loss=1.20]\n",
            "[Batch=030] [Loss=1.07]\n",
            "\n",
            "==>>[2023-04-30 15:55:00] [Epoch=010/030] [KMeansSampling Need: 00:03:05] [LR=0.1000] [Best : Test Accuracy=0.53, Error=0.47]\n",
            "[Batch=000] [Loss=0.98]\n",
            "[Batch=010] [Loss=1.00]\n",
            "[Batch=020] [Loss=1.07]\n",
            "[Batch=030] [Loss=1.03]\n",
            "\n",
            "==>>[2023-04-30 15:55:08] [Epoch=011/030] [KMeansSampling Need: 00:02:57] [LR=0.1000] [Best : Test Accuracy=0.55, Error=0.45]\n",
            "[Batch=000] [Loss=1.09]\n",
            "[Batch=010] [Loss=1.08]\n",
            "[Batch=020] [Loss=1.06]\n",
            "[Batch=030] [Loss=1.02]\n",
            "\n",
            "==>>[2023-04-30 15:55:18] [Epoch=012/030] [KMeansSampling Need: 00:02:46] [LR=0.1000] [Best : Test Accuracy=0.59, Error=0.41]\n",
            "[Batch=000] [Loss=0.94]\n",
            "[Batch=010] [Loss=0.94]\n",
            "[Batch=020] [Loss=0.93]\n",
            "[Batch=030] [Loss=0.96]\n",
            "\n",
            "==>>[2023-04-30 15:55:27] [Epoch=013/030] [KMeansSampling Need: 00:02:37] [LR=0.1000] [Best : Test Accuracy=0.61, Error=0.39]\n",
            "[Batch=000] [Loss=0.90]\n",
            "[Batch=010] [Loss=0.96]\n",
            "[Batch=020] [Loss=0.96]\n",
            "[Batch=030] [Loss=0.93]\n",
            "\n",
            "==>>[2023-04-30 15:55:36] [Epoch=014/030] [KMeansSampling Need: 00:02:28] [LR=0.1000] [Best : Test Accuracy=0.61, Error=0.39]\n",
            "[Batch=000] [Loss=0.91]\n",
            "[Batch=010] [Loss=1.00]\n",
            "[Batch=020] [Loss=1.00]\n",
            "[Batch=030] [Loss=0.82]\n",
            "\n",
            "==>>[2023-04-30 15:55:45] [Epoch=015/030] [KMeansSampling Need: 00:02:18] [LR=0.1000] [Best : Test Accuracy=0.61, Error=0.39]\n",
            "[Batch=000] [Loss=0.85]\n",
            "[Batch=010] [Loss=0.90]\n",
            "[Batch=020] [Loss=0.77]\n",
            "[Batch=030] [Loss=0.88]\n",
            "\n",
            "==>>[2023-04-30 15:55:55] [Epoch=016/030] [KMeansSampling Need: 00:02:09] [LR=0.1000] [Best : Test Accuracy=0.62, Error=0.38]\n",
            "[Batch=000] [Loss=0.79]\n",
            "[Batch=010] [Loss=0.80]\n",
            "[Batch=020] [Loss=0.84]\n",
            "[Batch=030] [Loss=0.84]\n",
            "\n",
            "==>>[2023-04-30 15:56:03] [Epoch=017/030] [KMeansSampling Need: 00:02:00] [LR=0.1000] [Best : Test Accuracy=0.66, Error=0.34]\n",
            "[Batch=000] [Loss=0.96]\n",
            "[Batch=010] [Loss=1.06]\n",
            "[Batch=020] [Loss=0.81]\n",
            "[Batch=030] [Loss=0.81]\n",
            "\n",
            "==>>[2023-04-30 15:56:13] [Epoch=018/030] [KMeansSampling Need: 00:01:50] [LR=0.1000] [Best : Test Accuracy=0.66, Error=0.34]\n",
            "[Batch=000] [Loss=0.85]\n",
            "[Batch=010] [Loss=0.95]\n",
            "[Batch=020] [Loss=0.93]\n",
            "[Batch=030] [Loss=0.76]\n",
            "\n",
            "==>>[2023-04-30 15:56:22] [Epoch=019/030] [KMeansSampling Need: 00:01:41] [LR=0.1000] [Best : Test Accuracy=0.66, Error=0.34]\n",
            "[Batch=000] [Loss=0.73]\n",
            "[Batch=010] [Loss=0.82]\n",
            "[Batch=020] [Loss=0.81]\n",
            "[Batch=030] [Loss=0.70]\n",
            "\n",
            "==>>[2023-04-30 15:56:31] [Epoch=020/030] [KMeansSampling Need: 00:01:32] [LR=0.1000] [Best : Test Accuracy=0.66, Error=0.34]\n",
            "[Batch=000] [Loss=0.69]\n",
            "[Batch=010] [Loss=0.79]\n",
            "[Batch=020] [Loss=0.77]\n",
            "[Batch=030] [Loss=0.68]\n",
            "\n",
            "==>>[2023-04-30 15:56:40] [Epoch=021/030] [KMeansSampling Need: 00:01:23] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.69]\n",
            "[Batch=010] [Loss=0.72]\n",
            "[Batch=020] [Loss=0.63]\n",
            "[Batch=030] [Loss=0.83]\n",
            "\n",
            "==>>[2023-04-30 15:56:49] [Epoch=022/030] [KMeansSampling Need: 00:01:13] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.63]\n",
            "[Batch=010] [Loss=0.70]\n",
            "[Batch=020] [Loss=0.70]\n",
            "[Batch=030] [Loss=0.58]\n",
            "\n",
            "==>>[2023-04-30 15:56:58] [Epoch=023/030] [KMeansSampling Need: 00:01:04] [LR=0.1000] [Best : Test Accuracy=0.71, Error=0.29]\n",
            "[Batch=000] [Loss=0.68]\n",
            "[Batch=010] [Loss=0.79]\n",
            "[Batch=020] [Loss=0.88]\n",
            "[Batch=030] [Loss=0.68]\n",
            "\n",
            "==>>[2023-04-30 15:57:07] [Epoch=024/030] [KMeansSampling Need: 00:00:55] [LR=0.1000] [Best : Test Accuracy=0.71, Error=0.29]\n",
            "[Batch=000] [Loss=0.68]\n",
            "[Batch=010] [Loss=0.72]\n",
            "[Batch=020] [Loss=0.80]\n",
            "[Batch=030] [Loss=0.59]\n",
            "\n",
            "==>>[2023-04-30 15:57:16] [Epoch=025/030] [KMeansSampling Need: 00:00:45] [LR=0.1000] [Best : Test Accuracy=0.71, Error=0.29]\n",
            "[Batch=000] [Loss=0.63]\n",
            "[Batch=010] [Loss=0.65]\n",
            "[Batch=020] [Loss=0.68]\n",
            "[Batch=030] [Loss=0.58]\n",
            "\n",
            "==>>[2023-04-30 15:57:24] [Epoch=026/030] [KMeansSampling Need: 00:00:36] [LR=0.1000] [Best : Test Accuracy=0.71, Error=0.29]\n",
            "[Batch=000] [Loss=0.68]\n",
            "[Batch=010] [Loss=0.63]\n",
            "[Batch=020] [Loss=0.53]\n",
            "[Batch=030] [Loss=0.62]\n",
            "\n",
            "==>>[2023-04-30 15:57:34] [Epoch=027/030] [KMeansSampling Need: 00:00:27] [LR=0.1000] [Best : Test Accuracy=0.71, Error=0.29]\n",
            "[Batch=000] [Loss=0.58]\n",
            "[Batch=010] [Loss=0.84]\n",
            "[Batch=020] [Loss=0.80]\n",
            "[Batch=030] [Loss=0.77]\n",
            "\n",
            "==>>[2023-04-30 15:57:43] [Epoch=028/030] [KMeansSampling Need: 00:00:18] [LR=0.1000] [Best : Test Accuracy=0.71, Error=0.29]\n",
            "[Batch=000] [Loss=0.58]\n",
            "[Batch=010] [Loss=0.57]\n",
            "[Batch=020] [Loss=0.56]\n",
            "[Batch=030] [Loss=0.60]\n",
            "\n",
            "==>>[2023-04-30 15:57:51] [Epoch=029/030] [KMeansSampling Need: 00:00:09] [LR=0.1000] [Best : Test Accuracy=0.73, Error=0.27]\n",
            "---- save figure the accuracy/loss curve of train/val into ./save/cifar10\n",
            "==>> Testing accuracy 0.724\n",
            "success!\n"
          ]
        }
      ],
      "source": [
        "!python /content/deep-active-learning/main.py --model ResNet18  --nStart 20  --dataset cifar10 --strategy KMeansSampling --nEnd=20 --n_epoch=30 --seed 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bwu2nSaB3ju8",
        "outputId": "83b5150f-4e47-4ddd-d1c7-80b64893b8f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "save path : ./save\n",
            "{'strategy': 'LeastConfidence', 'nQuery': 1, 'nStart': 20.0, 'nEnd': 20.0, 'nEmb': 256, 'seed': 1, 'model': 'ResNet18', 'dataset': 'cifar10', 'data_path': './datasets', 'save_path': './save', 'save_file': 'result.csv', 'hidden_units': 128, 'dropout_rate': 0.3, 'lambda_loss': 1.2, 's_margin': 0.1, 'n_ensembles': 1, 'proxy_model': None, 'optimizer': 'SGD', 'n_epoch': 30, 'schedule': [80, 120], 'momentum': 0.9, 'lr': 0.1, 'gammas': [0.1, 0.1], 'save_model': False, 'load_ckpt': False, 'add_imagenet': False}\n",
            "Random Seed: 1\n",
            "python version : 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]\n",
            "torch  version : 2.0.0+cu118\n",
            "cudnn  version : 8700\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[init=10000] [query=500] [end=10000]\n",
            "Strategy LeastConfidence successfully loaded...\n",
            "Let's use 1 GPUs!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "[Batch=000] [Loss=2.36]\n",
            "[Batch=010] [Loss=2.20]\n",
            "[Batch=020] [Loss=1.93]\n",
            "[Batch=030] [Loss=1.92]\n",
            "\n",
            "==>>[2023-04-30 15:58:14] [Epoch=000/030] [LeastConfidence Need: 00:00:00] [LR=0.1000] [Best : Test Accuracy=0.00, Error=1.00]\n",
            "[Batch=000] [Loss=2.10]\n",
            "[Batch=010] [Loss=1.77]\n",
            "[Batch=020] [Loss=1.78]\n",
            "[Batch=030] [Loss=1.74]\n",
            "\n",
            "==>>[2023-04-30 15:58:23] [Epoch=001/030] [LeastConfidence Need: 00:05:14] [LR=0.1000] [Best : Test Accuracy=0.20, Error=0.80]\n",
            "[Batch=000] [Loss=1.72]\n",
            "[Batch=010] [Loss=1.78]\n",
            "[Batch=020] [Loss=1.68]\n",
            "[Batch=030] [Loss=1.62]\n",
            "\n",
            "==>>[2023-04-30 15:58:31] [Epoch=002/030] [LeastConfidence Need: 00:04:40] [LR=0.1000] [Best : Test Accuracy=0.31, Error=0.69]\n",
            "[Batch=000] [Loss=1.70]\n",
            "[Batch=010] [Loss=1.50]\n",
            "[Batch=020] [Loss=1.60]\n",
            "[Batch=030] [Loss=1.60]\n",
            "\n",
            "==>>[2023-04-30 15:58:40] [Epoch=003/030] [LeastConfidence Need: 00:04:12] [LR=0.1000] [Best : Test Accuracy=0.36, Error=0.64]\n",
            "[Batch=000] [Loss=1.54]\n",
            "[Batch=010] [Loss=1.60]\n",
            "[Batch=020] [Loss=1.55]\n",
            "[Batch=030] [Loss=1.40]\n",
            "\n",
            "==>>[2023-04-30 15:58:50] [Epoch=004/030] [LeastConfidence Need: 00:04:03] [LR=0.1000] [Best : Test Accuracy=0.42, Error=0.58]\n",
            "[Batch=000] [Loss=1.61]\n",
            "[Batch=010] [Loss=1.48]\n",
            "[Batch=020] [Loss=1.48]\n",
            "[Batch=030] [Loss=1.41]\n",
            "\n",
            "==>>[2023-04-30 15:58:58] [Epoch=005/030] [LeastConfidence Need: 00:03:53] [LR=0.1000] [Best : Test Accuracy=0.42, Error=0.58]\n",
            "[Batch=000] [Loss=1.34]\n",
            "[Batch=010] [Loss=1.42]\n",
            "[Batch=020] [Loss=1.38]\n",
            "[Batch=030] [Loss=1.34]\n",
            "\n",
            "==>>[2023-04-30 15:59:07] [Epoch=006/030] [LeastConfidence Need: 00:03:39] [LR=0.1000] [Best : Test Accuracy=0.46, Error=0.54]\n",
            "[Batch=000] [Loss=1.43]\n",
            "[Batch=010] [Loss=1.27]\n",
            "[Batch=020] [Loss=1.18]\n",
            "[Batch=030] [Loss=1.29]\n",
            "\n",
            "==>>[2023-04-30 15:59:16] [Epoch=007/030] [LeastConfidence Need: 00:03:30] [LR=0.1000] [Best : Test Accuracy=0.46, Error=0.54]\n",
            "[Batch=000] [Loss=1.29]\n",
            "[Batch=010] [Loss=1.33]\n",
            "[Batch=020] [Loss=1.14]\n",
            "[Batch=030] [Loss=1.12]\n",
            "\n",
            "==>>[2023-04-30 15:59:24] [Epoch=008/030] [LeastConfidence Need: 00:03:21] [LR=0.1000] [Best : Test Accuracy=0.48, Error=0.52]\n",
            "[Batch=000] [Loss=1.10]\n",
            "[Batch=010] [Loss=1.22]\n",
            "[Batch=020] [Loss=1.10]\n",
            "[Batch=030] [Loss=1.20]\n",
            "\n",
            "==>>[2023-04-30 15:59:34] [Epoch=009/030] [LeastConfidence Need: 00:03:10] [LR=0.1000] [Best : Test Accuracy=0.55, Error=0.45]\n",
            "[Batch=000] [Loss=1.17]\n",
            "[Batch=010] [Loss=1.12]\n",
            "[Batch=020] [Loss=1.22]\n",
            "[Batch=030] [Loss=1.03]\n",
            "\n",
            "==>>[2023-04-30 15:59:43] [Epoch=010/030] [LeastConfidence Need: 00:03:01] [LR=0.1000] [Best : Test Accuracy=0.56, Error=0.44]\n",
            "[Batch=000] [Loss=1.03]\n",
            "[Batch=010] [Loss=0.98]\n",
            "[Batch=020] [Loss=1.08]\n",
            "[Batch=030] [Loss=1.02]\n",
            "\n",
            "==>>[2023-04-30 15:59:51] [Epoch=011/030] [LeastConfidence Need: 00:02:52] [LR=0.1000] [Best : Test Accuracy=0.59, Error=0.41]\n",
            "[Batch=000] [Loss=1.11]\n",
            "[Batch=010] [Loss=1.03]\n",
            "[Batch=020] [Loss=1.02]\n",
            "[Batch=030] [Loss=1.12]\n",
            "\n",
            "==>>[2023-04-30 16:00:00] [Epoch=012/030] [LeastConfidence Need: 00:02:42] [LR=0.1000] [Best : Test Accuracy=0.59, Error=0.41]\n",
            "[Batch=000] [Loss=0.96]\n",
            "[Batch=010] [Loss=0.93]\n",
            "[Batch=020] [Loss=0.94]\n",
            "[Batch=030] [Loss=1.01]\n",
            "\n",
            "==>>[2023-04-30 16:00:10] [Epoch=013/030] [LeastConfidence Need: 00:02:33] [LR=0.1000] [Best : Test Accuracy=0.60, Error=0.40]\n",
            "[Batch=000] [Loss=0.96]\n",
            "[Batch=010] [Loss=0.91]\n",
            "[Batch=020] [Loss=0.84]\n",
            "[Batch=030] [Loss=0.86]\n",
            "\n",
            "==>>[2023-04-30 16:00:18] [Epoch=014/030] [LeastConfidence Need: 00:02:24] [LR=0.1000] [Best : Test Accuracy=0.60, Error=0.40]\n",
            "[Batch=000] [Loss=0.95]\n",
            "[Batch=010] [Loss=1.03]\n",
            "[Batch=020] [Loss=0.97]\n",
            "[Batch=030] [Loss=0.85]\n",
            "\n",
            "==>>[2023-04-30 16:00:27] [Epoch=015/030] [LeastConfidence Need: 00:02:14] [LR=0.1000] [Best : Test Accuracy=0.60, Error=0.40]\n",
            "[Batch=000] [Loss=0.80]\n",
            "[Batch=010] [Loss=0.91]\n",
            "[Batch=020] [Loss=0.79]\n",
            "[Batch=030] [Loss=0.87]\n",
            "\n",
            "==>>[2023-04-30 16:00:36] [Epoch=016/030] [LeastConfidence Need: 00:02:06] [LR=0.1000] [Best : Test Accuracy=0.62, Error=0.38]\n",
            "[Batch=000] [Loss=0.85]\n",
            "[Batch=010] [Loss=0.83]\n",
            "[Batch=020] [Loss=0.85]\n",
            "[Batch=030] [Loss=0.92]\n",
            "\n",
            "==>>[2023-04-30 16:00:44] [Epoch=017/030] [LeastConfidence Need: 00:01:57] [LR=0.1000] [Best : Test Accuracy=0.62, Error=0.38]\n",
            "[Batch=000] [Loss=1.04]\n",
            "[Batch=010] [Loss=0.92]\n",
            "[Batch=020] [Loss=0.71]\n",
            "[Batch=030] [Loss=0.80]\n",
            "\n",
            "==>>[2023-04-30 16:00:54] [Epoch=018/030] [LeastConfidence Need: 00:01:47] [LR=0.1000] [Best : Test Accuracy=0.62, Error=0.38]\n",
            "[Batch=000] [Loss=0.80]\n",
            "[Batch=010] [Loss=0.94]\n",
            "[Batch=020] [Loss=0.89]\n",
            "[Batch=030] [Loss=0.73]\n",
            "\n",
            "==>>[2023-04-30 16:01:03] [Epoch=019/030] [LeastConfidence Need: 00:01:38] [LR=0.1000] [Best : Test Accuracy=0.66, Error=0.34]\n",
            "[Batch=000] [Loss=0.74]\n",
            "[Batch=010] [Loss=0.84]\n",
            "[Batch=020] [Loss=0.74]\n",
            "[Batch=030] [Loss=0.73]\n",
            "\n",
            "==>>[2023-04-30 16:01:11] [Epoch=020/030] [LeastConfidence Need: 00:01:29] [LR=0.1000] [Best : Test Accuracy=0.66, Error=0.34]\n",
            "[Batch=000] [Loss=0.68]\n",
            "[Batch=010] [Loss=0.80]\n",
            "[Batch=020] [Loss=0.74]\n",
            "[Batch=030] [Loss=0.69]\n",
            "\n",
            "==>>[2023-04-30 16:01:20] [Epoch=021/030] [LeastConfidence Need: 00:01:20] [LR=0.1000] [Best : Test Accuracy=0.72, Error=0.28]\n",
            "[Batch=000] [Loss=0.64]\n",
            "[Batch=010] [Loss=0.71]\n",
            "[Batch=020] [Loss=0.58]\n",
            "[Batch=030] [Loss=0.80]\n",
            "\n",
            "==>>[2023-04-30 16:01:30] [Epoch=022/030] [LeastConfidence Need: 00:01:11] [LR=0.1000] [Best : Test Accuracy=0.72, Error=0.28]\n",
            "[Batch=000] [Loss=0.61]\n",
            "[Batch=010] [Loss=0.70]\n",
            "[Batch=020] [Loss=0.69]\n",
            "[Batch=030] [Loss=0.62]\n",
            "\n",
            "==>>[2023-04-30 16:01:38] [Epoch=023/030] [LeastConfidence Need: 00:01:02] [LR=0.1000] [Best : Test Accuracy=0.72, Error=0.28]\n",
            "[Batch=000] [Loss=0.63]\n",
            "[Batch=010] [Loss=0.79]\n",
            "[Batch=020] [Loss=0.88]\n",
            "[Batch=030] [Loss=0.62]\n",
            "\n",
            "==>>[2023-04-30 16:01:47] [Epoch=024/030] [LeastConfidence Need: 00:00:53] [LR=0.1000] [Best : Test Accuracy=0.72, Error=0.28]\n",
            "[Batch=000] [Loss=0.69]\n",
            "[Batch=010] [Loss=0.84]\n",
            "[Batch=020] [Loss=0.77]\n",
            "[Batch=030] [Loss=0.67]\n",
            "\n",
            "==>>[2023-04-30 16:01:56] [Epoch=025/030] [LeastConfidence Need: 00:00:44] [LR=0.1000] [Best : Test Accuracy=0.72, Error=0.28]\n",
            "[Batch=000] [Loss=0.70]\n",
            "[Batch=010] [Loss=0.65]\n",
            "[Batch=020] [Loss=0.58]\n",
            "[Batch=030] [Loss=0.55]\n",
            "\n",
            "==>>[2023-04-30 16:02:04] [Epoch=026/030] [LeastConfidence Need: 00:00:35] [LR=0.1000] [Best : Test Accuracy=0.72, Error=0.28]\n",
            "[Batch=000] [Loss=0.68]\n",
            "[Batch=010] [Loss=0.63]\n",
            "[Batch=020] [Loss=0.58]\n",
            "[Batch=030] [Loss=0.63]\n",
            "\n",
            "==>>[2023-04-30 16:02:14] [Epoch=027/030] [LeastConfidence Need: 00:00:26] [LR=0.1000] [Best : Test Accuracy=0.72, Error=0.28]\n",
            "[Batch=000] [Loss=0.59]\n",
            "[Batch=010] [Loss=0.73]\n",
            "[Batch=020] [Loss=0.73]\n",
            "[Batch=030] [Loss=0.71]\n",
            "\n",
            "==>>[2023-04-30 16:02:23] [Epoch=028/030] [LeastConfidence Need: 00:00:17] [LR=0.1000] [Best : Test Accuracy=0.72, Error=0.28]\n",
            "[Batch=000] [Loss=0.61]\n",
            "[Batch=010] [Loss=0.51]\n",
            "[Batch=020] [Loss=0.54]\n",
            "[Batch=030] [Loss=0.57]\n",
            "\n",
            "==>>[2023-04-30 16:02:31] [Epoch=029/030] [LeastConfidence Need: 00:00:08] [LR=0.1000] [Best : Test Accuracy=0.72, Error=0.28]\n",
            "---- save figure the accuracy/loss curve of train/val into ./save/cifar10\n",
            "==>> Testing accuracy 0.6845\n",
            "success!\n"
          ]
        }
      ],
      "source": [
        "!python /content/deep-active-learning/main.py --model ResNet18  --nStart 20  --dataset cifar10 --strategy LeastConfidence --nEnd=20 --n_epoch=30 --seed 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt6rl8TG3r8u",
        "outputId": "13bf7a74-9b04-466f-e9c4-e692d8f09def"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "save path : ./save\n",
            "{'strategy': 'MarginSampling', 'nQuery': 1, 'nStart': 20.0, 'nEnd': 20.0, 'nEmb': 256, 'seed': 1, 'model': 'ResNet18', 'dataset': 'cifar10', 'data_path': './datasets', 'save_path': './save', 'save_file': 'result.csv', 'hidden_units': 128, 'dropout_rate': 0.3, 'lambda_loss': 1.2, 's_margin': 0.1, 'n_ensembles': 1, 'proxy_model': None, 'optimizer': 'SGD', 'n_epoch': 30, 'schedule': [80, 120], 'momentum': 0.9, 'lr': 0.1, 'gammas': [0.1, 0.1], 'save_model': False, 'load_ckpt': False, 'add_imagenet': False}\n",
            "Random Seed: 1\n",
            "python version : 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]\n",
            "torch  version : 2.0.0+cu118\n",
            "cudnn  version : 8700\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[init=10000] [query=500] [end=10000]\n",
            "Strategy MarginSampling successfully loaded...\n",
            "Let's use 1 GPUs!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "[Batch=000] [Loss=2.36]\n",
            "[Batch=010] [Loss=2.13]\n",
            "[Batch=020] [Loss=1.86]\n",
            "[Batch=030] [Loss=1.87]\n",
            "\n",
            "==>>[2023-04-30 16:02:54] [Epoch=000/030] [MarginSampling Need: 00:00:00] [LR=0.1000] [Best : Test Accuracy=0.00, Error=1.00]\n",
            "[Batch=000] [Loss=2.00]\n",
            "[Batch=010] [Loss=1.69]\n",
            "[Batch=020] [Loss=1.64]\n",
            "[Batch=030] [Loss=1.65]\n",
            "\n",
            "==>>[2023-04-30 16:03:03] [Epoch=001/030] [MarginSampling Need: 00:05:14] [LR=0.1000] [Best : Test Accuracy=0.22, Error=0.78]\n",
            "[Batch=000] [Loss=1.71]\n",
            "[Batch=010] [Loss=1.69]\n",
            "[Batch=020] [Loss=1.60]\n",
            "[Batch=030] [Loss=1.57]\n",
            "\n",
            "==>>[2023-04-30 16:03:12] [Epoch=002/030] [MarginSampling Need: 00:04:39] [LR=0.1000] [Best : Test Accuracy=0.31, Error=0.69]\n",
            "[Batch=000] [Loss=1.63]\n",
            "[Batch=010] [Loss=1.51]\n",
            "[Batch=020] [Loss=1.50]\n",
            "[Batch=030] [Loss=1.50]\n",
            "\n",
            "==>>[2023-04-30 16:03:21] [Epoch=003/030] [MarginSampling Need: 00:04:13] [LR=0.1000] [Best : Test Accuracy=0.41, Error=0.59]\n",
            "[Batch=000] [Loss=1.48]\n",
            "[Batch=010] [Loss=1.51]\n",
            "[Batch=020] [Loss=1.54]\n",
            "[Batch=030] [Loss=1.39]\n",
            "\n",
            "==>>[2023-04-30 16:03:30] [Epoch=004/030] [MarginSampling Need: 00:04:03] [LR=0.1000] [Best : Test Accuracy=0.45, Error=0.55]\n",
            "[Batch=000] [Loss=1.46]\n",
            "[Batch=010] [Loss=1.39]\n",
            "[Batch=020] [Loss=1.41]\n",
            "[Batch=030] [Loss=1.29]\n",
            "\n",
            "==>>[2023-04-30 16:03:38] [Epoch=005/030] [MarginSampling Need: 00:03:53] [LR=0.1000] [Best : Test Accuracy=0.45, Error=0.55]\n",
            "[Batch=000] [Loss=1.38]\n",
            "[Batch=010] [Loss=1.41]\n",
            "[Batch=020] [Loss=1.35]\n",
            "[Batch=030] [Loss=1.29]\n",
            "\n",
            "==>>[2023-04-30 16:03:47] [Epoch=006/030] [MarginSampling Need: 00:03:39] [LR=0.1000] [Best : Test Accuracy=0.45, Error=0.55]\n",
            "[Batch=000] [Loss=1.25]\n",
            "[Batch=010] [Loss=1.34]\n",
            "[Batch=020] [Loss=1.18]\n",
            "[Batch=030] [Loss=1.21]\n",
            "\n",
            "==>>[2023-04-30 16:03:57] [Epoch=007/030] [MarginSampling Need: 00:03:30] [LR=0.1000] [Best : Test Accuracy=0.47, Error=0.53]\n",
            "[Batch=000] [Loss=1.27]\n",
            "[Batch=010] [Loss=1.26]\n",
            "[Batch=020] [Loss=1.14]\n",
            "[Batch=030] [Loss=1.17]\n",
            "\n",
            "==>>[2023-04-30 16:04:05] [Epoch=008/030] [MarginSampling Need: 00:03:20] [LR=0.1000] [Best : Test Accuracy=0.50, Error=0.50]\n",
            "[Batch=000] [Loss=1.15]\n",
            "[Batch=010] [Loss=1.19]\n",
            "[Batch=020] [Loss=1.14]\n",
            "[Batch=030] [Loss=1.30]\n",
            "\n",
            "==>>[2023-04-30 16:04:14] [Epoch=009/030] [MarginSampling Need: 00:03:09] [LR=0.1000] [Best : Test Accuracy=0.50, Error=0.50]\n",
            "[Batch=000] [Loss=1.14]\n",
            "[Batch=010] [Loss=1.15]\n",
            "[Batch=020] [Loss=1.26]\n",
            "[Batch=030] [Loss=1.05]\n",
            "\n",
            "==>>[2023-04-30 16:04:23] [Epoch=010/030] [MarginSampling Need: 00:03:00] [LR=0.1000] [Best : Test Accuracy=0.56, Error=0.44]\n",
            "[Batch=000] [Loss=1.02]\n",
            "[Batch=010] [Loss=1.00]\n",
            "[Batch=020] [Loss=1.10]\n",
            "[Batch=030] [Loss=1.00]\n",
            "\n",
            "==>>[2023-04-30 16:04:31] [Epoch=011/030] [MarginSampling Need: 00:02:52] [LR=0.1000] [Best : Test Accuracy=0.56, Error=0.44]\n",
            "[Batch=000] [Loss=1.08]\n",
            "[Batch=010] [Loss=1.03]\n",
            "[Batch=020] [Loss=1.05]\n",
            "[Batch=030] [Loss=0.98]\n",
            "\n",
            "==>>[2023-04-30 16:04:40] [Epoch=012/030] [MarginSampling Need: 00:02:41] [LR=0.1000] [Best : Test Accuracy=0.59, Error=0.41]\n",
            "[Batch=000] [Loss=0.96]\n",
            "[Batch=010] [Loss=0.97]\n",
            "[Batch=020] [Loss=0.97]\n",
            "[Batch=030] [Loss=1.05]\n",
            "\n",
            "==>>[2023-04-30 16:04:50] [Epoch=013/030] [MarginSampling Need: 00:02:32] [LR=0.1000] [Best : Test Accuracy=0.59, Error=0.41]\n",
            "[Batch=000] [Loss=0.88]\n",
            "[Batch=010] [Loss=0.96]\n",
            "[Batch=020] [Loss=0.91]\n",
            "[Batch=030] [Loss=0.86]\n",
            "\n",
            "==>>[2023-04-30 16:04:58] [Epoch=014/030] [MarginSampling Need: 00:02:24] [LR=0.1000] [Best : Test Accuracy=0.59, Error=0.41]\n",
            "[Batch=000] [Loss=0.92]\n",
            "[Batch=010] [Loss=1.09]\n",
            "[Batch=020] [Loss=1.03]\n",
            "[Batch=030] [Loss=0.80]\n",
            "\n",
            "==>>[2023-04-30 16:05:07] [Epoch=015/030] [MarginSampling Need: 00:02:14] [LR=0.1000] [Best : Test Accuracy=0.59, Error=0.41]\n",
            "[Batch=000] [Loss=0.84]\n",
            "[Batch=010] [Loss=0.95]\n",
            "[Batch=020] [Loss=0.81]\n",
            "[Batch=030] [Loss=0.91]\n",
            "\n",
            "==>>[2023-04-30 16:05:16] [Epoch=016/030] [MarginSampling Need: 00:02:05] [LR=0.1000] [Best : Test Accuracy=0.64, Error=0.36]\n",
            "[Batch=000] [Loss=0.81]\n",
            "[Batch=010] [Loss=0.75]\n",
            "[Batch=020] [Loss=0.79]\n",
            "[Batch=030] [Loss=0.81]\n",
            "\n",
            "==>>[2023-04-30 16:05:25] [Epoch=017/030] [MarginSampling Need: 00:01:57] [LR=0.1000] [Best : Test Accuracy=0.66, Error=0.34]\n",
            "[Batch=000] [Loss=0.99]\n",
            "[Batch=010] [Loss=0.95]\n",
            "[Batch=020] [Loss=0.79]\n",
            "[Batch=030] [Loss=0.78]\n",
            "\n",
            "==>>[2023-04-30 16:05:34] [Epoch=018/030] [MarginSampling Need: 00:01:47] [LR=0.1000] [Best : Test Accuracy=0.66, Error=0.34]\n",
            "[Batch=000] [Loss=0.84]\n",
            "[Batch=010] [Loss=0.98]\n",
            "[Batch=020] [Loss=0.85]\n",
            "[Batch=030] [Loss=0.71]\n",
            "\n",
            "==>>[2023-04-30 16:05:43] [Epoch=019/030] [MarginSampling Need: 00:01:38] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.79]\n",
            "[Batch=010] [Loss=0.83]\n",
            "[Batch=020] [Loss=0.77]\n",
            "[Batch=030] [Loss=0.73]\n",
            "\n",
            "==>>[2023-04-30 16:05:51] [Epoch=020/030] [MarginSampling Need: 00:01:29] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.68]\n",
            "[Batch=010] [Loss=0.83]\n",
            "[Batch=020] [Loss=0.80]\n",
            "[Batch=030] [Loss=0.73]\n",
            "\n",
            "==>>[2023-04-30 16:06:01] [Epoch=021/030] [MarginSampling Need: 00:01:20] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.73]\n",
            "[Batch=010] [Loss=0.76]\n",
            "[Batch=020] [Loss=0.65]\n",
            "[Batch=030] [Loss=0.86]\n",
            "\n",
            "==>>[2023-04-30 16:06:10] [Epoch=022/030] [MarginSampling Need: 00:01:11] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.64]\n",
            "[Batch=010] [Loss=0.73]\n",
            "[Batch=020] [Loss=0.74]\n",
            "[Batch=030] [Loss=0.62]\n",
            "\n",
            "==>>[2023-04-30 16:06:18] [Epoch=023/030] [MarginSampling Need: 00:01:02] [LR=0.1000] [Best : Test Accuracy=0.70, Error=0.30]\n",
            "[Batch=000] [Loss=0.65]\n",
            "[Batch=010] [Loss=0.71]\n",
            "[Batch=020] [Loss=0.83]\n",
            "[Batch=030] [Loss=0.64]\n",
            "\n",
            "==>>[2023-04-30 16:06:27] [Epoch=024/030] [MarginSampling Need: 00:00:53] [LR=0.1000] [Best : Test Accuracy=0.71, Error=0.29]\n",
            "[Batch=000] [Loss=0.71]\n",
            "[Batch=010] [Loss=0.86]\n",
            "[Batch=020] [Loss=0.76]\n",
            "[Batch=030] [Loss=0.63]\n",
            "\n",
            "==>>[2023-04-30 16:06:37] [Epoch=025/030] [MarginSampling Need: 00:00:44] [LR=0.1000] [Best : Test Accuracy=0.71, Error=0.29]\n",
            "[Batch=000] [Loss=0.64]\n",
            "[Batch=010] [Loss=0.64]\n",
            "[Batch=020] [Loss=0.68]\n",
            "[Batch=030] [Loss=0.57]\n",
            "\n",
            "==>>[2023-04-30 16:06:45] [Epoch=026/030] [MarginSampling Need: 00:00:35] [LR=0.1000] [Best : Test Accuracy=0.71, Error=0.29]\n",
            "[Batch=000] [Loss=0.66]\n",
            "[Batch=010] [Loss=0.63]\n",
            "[Batch=020] [Loss=0.55]\n",
            "[Batch=030] [Loss=0.62]\n",
            "\n",
            "==>>[2023-04-30 16:06:54] [Epoch=027/030] [MarginSampling Need: 00:00:26] [LR=0.1000] [Best : Test Accuracy=0.71, Error=0.29]\n",
            "[Batch=000] [Loss=0.59]\n",
            "[Batch=010] [Loss=0.77]\n",
            "[Batch=020] [Loss=0.77]\n",
            "[Batch=030] [Loss=0.68]\n",
            "\n",
            "==>>[2023-04-30 16:07:04] [Epoch=028/030] [MarginSampling Need: 00:00:17] [LR=0.1000] [Best : Test Accuracy=0.71, Error=0.29]\n",
            "[Batch=000] [Loss=0.58]\n",
            "[Batch=010] [Loss=0.47]\n",
            "[Batch=020] [Loss=0.56]\n",
            "[Batch=030] [Loss=0.57]\n",
            "\n",
            "==>>[2023-04-30 16:07:12] [Epoch=029/030] [MarginSampling Need: 00:00:08] [LR=0.1000] [Best : Test Accuracy=0.71, Error=0.29]\n",
            "---- save figure the accuracy/loss curve of train/val into ./save/cifar10\n",
            "==>> Testing accuracy 0.7429\n",
            "success!\n"
          ]
        }
      ],
      "source": [
        "!python /content/deep-active-learning/main.py --model ResNet18  --nStart 20  --dataset cifar10 --strategy MarginSampling --nEnd=20 --n_epoch=30 --seed 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_etStNhJ3zL-",
        "outputId": "7ecbccad-e11a-446a-be80-1f5a946e059b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "save path : ./save\n",
            "{'strategy': 'RandomSampling', 'nQuery': 1, 'nStart': 20.0, 'nEnd': 20.0, 'nEmb': 256, 'seed': 1, 'model': 'ResNet18', 'dataset': 'cifar10', 'data_path': './datasets', 'save_path': './save', 'save_file': 'result.csv', 'hidden_units': 128, 'dropout_rate': 0.3, 'lambda_loss': 1.2, 's_margin': 0.1, 'n_ensembles': 1, 'proxy_model': None, 'optimizer': 'SGD', 'n_epoch': 30, 'schedule': [80, 120], 'momentum': 0.9, 'lr': 0.1, 'gammas': [0.1, 0.1], 'save_model': False, 'load_ckpt': False, 'add_imagenet': False}\n",
            "Random Seed: 1\n",
            "python version : 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]\n",
            "torch  version : 2.0.0+cu118\n",
            "cudnn  version : 8700\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[init=10000] [query=500] [end=10000]\n",
            "Strategy RandomSampling successfully loaded...\n",
            "Let's use 1 GPUs!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "[Batch=000] [Loss=2.36]\n",
            "[Batch=010] [Loss=2.27]\n",
            "[Batch=020] [Loss=1.97]\n",
            "[Batch=030] [Loss=1.93]\n",
            "\n",
            "==>>[2023-04-30 16:07:35] [Epoch=000/030] [RandomSampling Need: 00:00:00] [LR=0.1000] [Best : Test Accuracy=0.00, Error=1.00]\n",
            "[Batch=000] [Loss=2.02]\n",
            "[Batch=010] [Loss=1.65]\n",
            "[Batch=020] [Loss=1.65]\n",
            "[Batch=030] [Loss=1.66]\n",
            "\n",
            "==>>[2023-04-30 16:07:45] [Epoch=001/030] [RandomSampling Need: 00:05:12] [LR=0.1000] [Best : Test Accuracy=0.25, Error=0.75]\n",
            "[Batch=000] [Loss=1.64]\n",
            "[Batch=010] [Loss=1.78]\n",
            "[Batch=020] [Loss=1.59]\n",
            "[Batch=030] [Loss=1.53]\n",
            "\n",
            "==>>[2023-04-30 16:07:53] [Epoch=002/030] [RandomSampling Need: 00:04:40] [LR=0.1000] [Best : Test Accuracy=0.35, Error=0.65]\n",
            "[Batch=000] [Loss=1.52]\n",
            "[Batch=010] [Loss=1.52]\n",
            "[Batch=020] [Loss=1.52]\n",
            "[Batch=030] [Loss=1.48]\n",
            "\n",
            "==>>[2023-04-30 16:08:03] [Epoch=003/030] [RandomSampling Need: 00:04:15] [LR=0.1000] [Best : Test Accuracy=0.36, Error=0.64]\n",
            "[Batch=000] [Loss=1.54]\n",
            "[Batch=010] [Loss=1.49]\n",
            "[Batch=020] [Loss=1.44]\n",
            "[Batch=030] [Loss=1.31]\n",
            "\n",
            "==>>[2023-04-30 16:08:11] [Epoch=004/030] [RandomSampling Need: 00:04:06] [LR=0.1000] [Best : Test Accuracy=0.44, Error=0.56]\n",
            "[Batch=000] [Loss=1.45]\n",
            "[Batch=010] [Loss=1.36]\n",
            "[Batch=020] [Loss=1.46]\n",
            "[Batch=030] [Loss=1.34]\n",
            "\n",
            "==>>[2023-04-30 16:08:20] [Epoch=005/030] [RandomSampling Need: 00:03:54] [LR=0.1000] [Best : Test Accuracy=0.44, Error=0.56]\n",
            "[Batch=000] [Loss=1.27]\n",
            "[Batch=010] [Loss=1.28]\n",
            "[Batch=020] [Loss=1.30]\n",
            "[Batch=030] [Loss=1.21]\n",
            "\n",
            "==>>[2023-04-30 16:08:30] [Epoch=006/030] [RandomSampling Need: 00:03:42] [LR=0.1000] [Best : Test Accuracy=0.48, Error=0.52]\n",
            "[Batch=000] [Loss=1.26]\n",
            "[Batch=010] [Loss=1.22]\n",
            "[Batch=020] [Loss=1.09]\n",
            "[Batch=030] [Loss=1.14]\n",
            "\n",
            "==>>[2023-04-30 16:08:38] [Epoch=007/030] [RandomSampling Need: 00:03:34] [LR=0.1000] [Best : Test Accuracy=0.50, Error=0.50]\n",
            "[Batch=000] [Loss=1.17]\n",
            "[Batch=010] [Loss=1.25]\n",
            "[Batch=020] [Loss=1.08]\n",
            "[Batch=030] [Loss=1.08]\n",
            "\n",
            "==>>[2023-04-30 16:08:47] [Epoch=008/030] [RandomSampling Need: 00:03:22] [LR=0.1000] [Best : Test Accuracy=0.54, Error=0.46]\n",
            "[Batch=000] [Loss=1.08]\n",
            "[Batch=010] [Loss=1.04]\n",
            "[Batch=020] [Loss=1.03]\n",
            "[Batch=030] [Loss=1.17]\n",
            "\n",
            "==>>[2023-04-30 16:08:57] [Epoch=009/030] [RandomSampling Need: 00:03:13] [LR=0.1000] [Best : Test Accuracy=0.55, Error=0.45]\n",
            "[Batch=000] [Loss=1.00]\n",
            "[Batch=010] [Loss=1.08]\n",
            "[Batch=020] [Loss=1.14]\n",
            "[Batch=030] [Loss=0.96]\n",
            "\n",
            "==>>[2023-04-30 16:09:05] [Epoch=010/030] [RandomSampling Need: 00:03:04] [LR=0.1000] [Best : Test Accuracy=0.59, Error=0.41]\n",
            "[Batch=000] [Loss=0.91]\n",
            "[Batch=010] [Loss=0.88]\n",
            "[Batch=020] [Loss=1.00]\n",
            "[Batch=030] [Loss=0.95]\n",
            "\n",
            "==>>[2023-04-30 16:09:14] [Epoch=011/030] [RandomSampling Need: 00:02:54] [LR=0.1000] [Best : Test Accuracy=0.63, Error=0.37]\n",
            "[Batch=000] [Loss=1.02]\n",
            "[Batch=010] [Loss=0.99]\n",
            "[Batch=020] [Loss=1.02]\n",
            "[Batch=030] [Loss=1.01]\n",
            "\n",
            "==>>[2023-04-30 16:09:24] [Epoch=012/030] [RandomSampling Need: 00:02:44] [LR=0.1000] [Best : Test Accuracy=0.63, Error=0.37]\n",
            "[Batch=000] [Loss=0.86]\n",
            "[Batch=010] [Loss=0.89]\n",
            "[Batch=020] [Loss=0.90]\n",
            "[Batch=030] [Loss=0.94]\n",
            "\n",
            "==>>[2023-04-30 16:09:32] [Epoch=013/030] [RandomSampling Need: 00:02:36] [LR=0.1000] [Best : Test Accuracy=0.63, Error=0.37]\n",
            "[Batch=000] [Loss=0.78]\n",
            "[Batch=010] [Loss=0.93]\n",
            "[Batch=020] [Loss=0.88]\n",
            "[Batch=030] [Loss=0.75]\n",
            "\n",
            "==>>[2023-04-30 16:09:41] [Epoch=014/030] [RandomSampling Need: 00:02:25] [LR=0.1000] [Best : Test Accuracy=0.63, Error=0.37]\n",
            "[Batch=000] [Loss=0.90]\n",
            "[Batch=010] [Loss=0.92]\n",
            "[Batch=020] [Loss=0.97]\n",
            "[Batch=030] [Loss=0.79]\n",
            "\n",
            "==>>[2023-04-30 16:09:51] [Epoch=015/030] [RandomSampling Need: 00:02:16] [LR=0.1000] [Best : Test Accuracy=0.63, Error=0.37]\n",
            "[Batch=000] [Loss=0.79]\n",
            "[Batch=010] [Loss=0.89]\n",
            "[Batch=020] [Loss=0.72]\n",
            "[Batch=030] [Loss=0.84]\n",
            "\n",
            "==>>[2023-04-30 16:09:59] [Epoch=016/030] [RandomSampling Need: 00:02:08] [LR=0.1000] [Best : Test Accuracy=0.63, Error=0.37]\n",
            "[Batch=000] [Loss=0.77]\n",
            "[Batch=010] [Loss=0.76]\n",
            "[Batch=020] [Loss=0.76]\n",
            "[Batch=030] [Loss=0.82]\n",
            "\n",
            "==>>[2023-04-30 16:10:09] [Epoch=017/030] [RandomSampling Need: 00:01:58] [LR=0.1000] [Best : Test Accuracy=0.63, Error=0.37]\n",
            "[Batch=000] [Loss=0.95]\n",
            "[Batch=010] [Loss=0.88]\n",
            "[Batch=020] [Loss=0.73]\n",
            "[Batch=030] [Loss=0.76]\n",
            "\n",
            "==>>[2023-04-30 16:10:18] [Epoch=018/030] [RandomSampling Need: 00:01:49] [LR=0.1000] [Best : Test Accuracy=0.63, Error=0.37]\n",
            "[Batch=000] [Loss=0.79]\n",
            "[Batch=010] [Loss=0.89]\n",
            "[Batch=020] [Loss=0.87]\n",
            "[Batch=030] [Loss=0.67]\n",
            "\n",
            "==>>[2023-04-30 16:10:26] [Epoch=019/030] [RandomSampling Need: 00:01:40] [LR=0.1000] [Best : Test Accuracy=0.65, Error=0.35]\n",
            "[Batch=000] [Loss=0.68]\n",
            "[Batch=010] [Loss=0.83]\n",
            "[Batch=020] [Loss=0.66]\n",
            "[Batch=030] [Loss=0.66]\n",
            "\n",
            "==>>[2023-04-30 16:10:35] [Epoch=020/030] [RandomSampling Need: 00:01:30] [LR=0.1000] [Best : Test Accuracy=0.66, Error=0.34]\n",
            "[Batch=000] [Loss=0.65]\n",
            "[Batch=010] [Loss=0.79]\n",
            "[Batch=020] [Loss=0.66]\n",
            "[Batch=030] [Loss=0.67]\n",
            "\n",
            "==>>[2023-04-30 16:10:45] [Epoch=021/030] [RandomSampling Need: 00:01:21] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.62]\n",
            "[Batch=010] [Loss=0.68]\n",
            "[Batch=020] [Loss=0.60]\n",
            "[Batch=030] [Loss=0.77]\n",
            "\n",
            "==>>[2023-04-30 16:10:53] [Epoch=022/030] [RandomSampling Need: 00:01:12] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.65]\n",
            "[Batch=010] [Loss=0.75]\n",
            "[Batch=020] [Loss=0.74]\n",
            "[Batch=030] [Loss=0.64]\n",
            "\n",
            "==>>[2023-04-30 16:11:02] [Epoch=023/030] [RandomSampling Need: 00:01:03] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.65]\n",
            "[Batch=010] [Loss=0.75]\n",
            "[Batch=020] [Loss=0.79]\n",
            "[Batch=030] [Loss=0.63]\n",
            "\n",
            "==>>[2023-04-30 16:11:12] [Epoch=024/030] [RandomSampling Need: 00:00:54] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.79]\n",
            "[Batch=010] [Loss=0.82]\n",
            "[Batch=020] [Loss=0.80]\n",
            "[Batch=030] [Loss=0.65]\n",
            "\n",
            "==>>[2023-04-30 16:11:20] [Epoch=025/030] [RandomSampling Need: 00:00:45] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.67]\n",
            "[Batch=010] [Loss=0.65]\n",
            "[Batch=020] [Loss=0.59]\n",
            "[Batch=030] [Loss=0.63]\n",
            "\n",
            "==>>[2023-04-30 16:11:29] [Epoch=026/030] [RandomSampling Need: 00:00:36] [LR=0.1000] [Best : Test Accuracy=0.70, Error=0.30]\n",
            "[Batch=000] [Loss=0.62]\n",
            "[Batch=010] [Loss=0.52]\n",
            "[Batch=020] [Loss=0.52]\n",
            "[Batch=030] [Loss=0.60]\n",
            "\n",
            "==>>[2023-04-30 16:11:39] [Epoch=027/030] [RandomSampling Need: 00:00:27] [LR=0.1000] [Best : Test Accuracy=0.72, Error=0.28]\n",
            "[Batch=000] [Loss=0.60]\n",
            "[Batch=010] [Loss=0.82]\n",
            "[Batch=020] [Loss=0.79]\n",
            "[Batch=030] [Loss=0.70]\n",
            "\n",
            "==>>[2023-04-30 16:11:47] [Epoch=028/030] [RandomSampling Need: 00:00:18] [LR=0.1000] [Best : Test Accuracy=0.72, Error=0.28]\n",
            "[Batch=000] [Loss=0.57]\n",
            "[Batch=010] [Loss=0.57]\n",
            "[Batch=020] [Loss=0.52]\n",
            "[Batch=030] [Loss=0.60]\n",
            "\n",
            "==>>[2023-04-30 16:11:57] [Epoch=029/030] [RandomSampling Need: 00:00:09] [LR=0.1000] [Best : Test Accuracy=0.72, Error=0.28]\n",
            "---- save figure the accuracy/loss curve of train/val into ./save/cifar10\n",
            "==>> Testing accuracy 0.6929\n",
            "success!\n"
          ]
        }
      ],
      "source": [
        "!python /content/deep-active-learning/main.py --model ResNet18  --nStart 20  --dataset cifar10 --strategy RandomSampling --nEnd=20 --n_epoch=30 --seed 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u24bEKl02E0P"
      },
      "source": [
        "# ***mnist Data***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXxWEjCo_VNj",
        "outputId": "ccce2981-5a63-497a-d892-57fa3bbd9e11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "save path : ./save\n",
            "{'strategy': 'EntropySampling', 'nQuery': 1, 'nStart': 20.0, 'nEnd': 20.0, 'nEmb': 256, 'seed': 1, 'model': 'ResNet18', 'dataset': 'cifar100', 'data_path': './datasets', 'save_path': './save', 'save_file': 'result.csv', 'hidden_units': 128, 'dropout_rate': 0.3, 'lambda_loss': 1.2, 's_margin': 0.1, 'n_ensembles': 1, 'proxy_model': None, 'optimizer': 'SGD', 'n_epoch': 30, 'schedule': [80, 120], 'momentum': 0.9, 'lr': 0.1, 'gammas': [0.1, 0.1], 'save_model': False, 'load_ckpt': False, 'add_imagenet': False}\n",
            "Random Seed: 1\n",
            "python version : 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]\n",
            "torch  version : 2.0.0+cu118\n",
            "cudnn  version : 8700\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./datasets/cifar100/cifar-100-python.tar.gz\n",
            "100% 169001437/169001437 [00:05<00:00, 28544549.59it/s]\n",
            "Extracting ./datasets/cifar100/cifar-100-python.tar.gz to ./datasets/cifar100\n",
            "Files already downloaded and verified\n",
            "[init=10000] [query=500] [end=10000]\n",
            "Strategy EntropySampling successfully loaded...\n",
            "Let's use 1 GPUs!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "[Batch=000] [Loss=4.72]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "==>>[2023-04-30 16:45:01] [Epoch=000/030] [EntropySampling Need: 00:00:00] [LR=0.1000] [Best : Test Accuracy=0.00, Error=1.00]\n",
            "[Batch=000] [Loss=4.46]\n",
            "\n",
            "==>>[2023-04-30 16:45:10] [Epoch=001/030] [EntropySampling Need: 00:05:14] [LR=0.1000] [Best : Test Accuracy=0.01, Error=0.99]\n",
            "[Batch=000] [Loss=4.40]\n",
            "\n",
            "==>>[2023-04-30 16:45:19] [Epoch=002/030] [EntropySampling Need: 00:04:32] [LR=0.1000] [Best : Test Accuracy=0.01, Error=0.99]\n",
            "[Batch=000] [Loss=4.25]\n",
            "\n",
            "==>>[2023-04-30 16:45:27] [Epoch=003/030] [EntropySampling Need: 00:04:14] [LR=0.1000] [Best : Test Accuracy=0.02, Error=0.98]\n",
            "[Batch=000] [Loss=4.10]\n",
            "\n",
            "==>>[2023-04-30 16:45:36] [Epoch=004/030] [EntropySampling Need: 00:03:56] [LR=0.1000] [Best : Test Accuracy=0.04, Error=0.96]\n",
            "[Batch=000] [Loss=3.96]\n",
            "\n",
            "==>>[2023-04-30 16:45:45] [Epoch=005/030] [EntropySampling Need: 00:03:47] [LR=0.1000] [Best : Test Accuracy=0.04, Error=0.96]\n",
            "[Batch=000] [Loss=3.82]\n",
            "\n",
            "==>>[2023-04-30 16:45:53] [Epoch=006/030] [EntropySampling Need: 00:03:36] [LR=0.1000] [Best : Test Accuracy=0.05, Error=0.95]\n",
            "[Batch=000] [Loss=3.75]\n",
            "\n",
            "==>>[2023-04-30 16:46:02] [Epoch=007/030] [EntropySampling Need: 00:03:24] [LR=0.1000] [Best : Test Accuracy=0.07, Error=0.93]\n",
            "[Batch=000] [Loss=3.66]\n",
            "\n",
            "==>>[2023-04-30 16:46:11] [Epoch=008/030] [EntropySampling Need: 00:03:16] [LR=0.1000] [Best : Test Accuracy=0.07, Error=0.93]\n",
            "[Batch=000] [Loss=3.62]\n",
            "\n",
            "==>>[2023-04-30 16:46:19] [Epoch=009/030] [EntropySampling Need: 00:03:08] [LR=0.1000] [Best : Test Accuracy=0.10, Error=0.90]\n",
            "[Batch=000] [Loss=3.55]\n",
            "\n",
            "==>>[2023-04-30 16:46:29] [Epoch=010/030] [EntropySampling Need: 00:02:57] [LR=0.1000] [Best : Test Accuracy=0.12, Error=0.88]\n",
            "[Batch=000] [Loss=3.46]\n",
            "\n",
            "==>>[2023-04-30 16:46:38] [Epoch=011/030] [EntropySampling Need: 00:02:49] [LR=0.1000] [Best : Test Accuracy=0.13, Error=0.87]\n",
            "[Batch=000] [Loss=3.38]\n",
            "\n",
            "==>>[2023-04-30 16:46:46] [Epoch=012/030] [EntropySampling Need: 00:02:41] [LR=0.1000] [Best : Test Accuracy=0.14, Error=0.86]\n",
            "[Batch=000] [Loss=3.28]\n",
            "\n",
            "==>>[2023-04-30 16:46:55] [Epoch=013/030] [EntropySampling Need: 00:02:31] [LR=0.1000] [Best : Test Accuracy=0.15, Error=0.85]\n",
            "[Batch=000] [Loss=3.32]\n",
            "\n",
            "==>>[2023-04-30 16:47:05] [Epoch=014/030] [EntropySampling Need: 00:02:22] [LR=0.1000] [Best : Test Accuracy=0.15, Error=0.85]\n",
            "[Batch=000] [Loss=3.23]\n",
            "\n",
            "==>>[2023-04-30 16:47:13] [Epoch=015/030] [EntropySampling Need: 00:02:14] [LR=0.1000] [Best : Test Accuracy=0.15, Error=0.85]\n",
            "[Batch=000] [Loss=3.21]\n",
            "\n",
            "==>>[2023-04-30 16:47:22] [Epoch=016/030] [EntropySampling Need: 00:02:04] [LR=0.1000] [Best : Test Accuracy=0.16, Error=0.84]\n",
            "[Batch=000] [Loss=3.10]\n",
            "\n",
            "==>>[2023-04-30 16:47:30] [Epoch=017/030] [EntropySampling Need: 00:01:55] [LR=0.1000] [Best : Test Accuracy=0.16, Error=0.84]\n",
            "[Batch=000] [Loss=3.02]\n",
            "\n",
            "==>>[2023-04-30 16:47:39] [Epoch=018/030] [EntropySampling Need: 00:01:46] [LR=0.1000] [Best : Test Accuracy=0.18, Error=0.82]\n",
            "[Batch=000] [Loss=2.97]\n",
            "\n",
            "==>>[2023-04-30 16:47:48] [Epoch=019/030] [EntropySampling Need: 00:01:37] [LR=0.1000] [Best : Test Accuracy=0.18, Error=0.82]\n",
            "[Batch=000] [Loss=2.94]\n",
            "\n",
            "==>>[2023-04-30 16:47:56] [Epoch=020/030] [EntropySampling Need: 00:01:28] [LR=0.1000] [Best : Test Accuracy=0.19, Error=0.81]\n",
            "[Batch=000] [Loss=2.85]\n",
            "\n",
            "==>>[2023-04-30 16:48:05] [Epoch=021/030] [EntropySampling Need: 00:01:19] [LR=0.1000] [Best : Test Accuracy=0.20, Error=0.80]\n",
            "[Batch=000] [Loss=2.74]\n",
            "\n",
            "==>>[2023-04-30 16:48:14] [Epoch=022/030] [EntropySampling Need: 00:01:10] [LR=0.1000] [Best : Test Accuracy=0.21, Error=0.79]\n",
            "[Batch=000] [Loss=2.73]\n",
            "\n",
            "==>>[2023-04-30 16:48:22] [Epoch=023/030] [EntropySampling Need: 00:01:02] [LR=0.1000] [Best : Test Accuracy=0.22, Error=0.78]\n",
            "[Batch=000] [Loss=2.66]\n",
            "\n",
            "==>>[2023-04-30 16:48:32] [Epoch=024/030] [EntropySampling Need: 00:00:53] [LR=0.1000] [Best : Test Accuracy=0.22, Error=0.78]\n",
            "[Batch=000] [Loss=2.60]\n",
            "\n",
            "==>>[2023-04-30 16:48:41] [Epoch=025/030] [EntropySampling Need: 00:00:44] [LR=0.1000] [Best : Test Accuracy=0.22, Error=0.78]\n",
            "[Batch=000] [Loss=2.55]\n",
            "\n",
            "==>>[2023-04-30 16:48:49] [Epoch=026/030] [EntropySampling Need: 00:00:35] [LR=0.1000] [Best : Test Accuracy=0.25, Error=0.75]\n",
            "[Batch=000] [Loss=2.59]\n",
            "\n",
            "==>>[2023-04-30 16:48:59] [Epoch=027/030] [EntropySampling Need: 00:00:26] [LR=0.1000] [Best : Test Accuracy=0.25, Error=0.75]\n",
            "[Batch=000] [Loss=2.47]\n",
            "\n",
            "==>>[2023-04-30 16:49:08] [Epoch=028/030] [EntropySampling Need: 00:00:17] [LR=0.1000] [Best : Test Accuracy=0.25, Error=0.75]\n",
            "[Batch=000] [Loss=2.43]\n",
            "\n",
            "==>>[2023-04-30 16:49:16] [Epoch=029/030] [EntropySampling Need: 00:00:08] [LR=0.1000] [Best : Test Accuracy=0.26, Error=0.74]\n",
            "---- save figure the accuracy/loss curve of train/val into ./save/cifar100\n",
            "==>> Testing accuracy 0.2587\n",
            "success!\n"
          ]
        }
      ],
      "source": [
        "!python /content/deep-active-learning/main.py --model ResNet18  --nStart 20  --dataset cifar100 --strategy EntropySampling --nEnd=20 --n_epoch=30 --seed 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE75l2zv_VNs",
        "outputId": "30dc70d7-3501-4675-f781-96816b7e66a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "save path : ./save\n",
            "{'strategy': 'KMeansSampling', 'nQuery': 1, 'nStart': 20.0, 'nEnd': 20.0, 'nEmb': 256, 'seed': 1, 'model': 'ResNet18', 'dataset': 'cifar100', 'data_path': './datasets', 'save_path': './save', 'save_file': 'result.csv', 'hidden_units': 128, 'dropout_rate': 0.3, 'lambda_loss': 1.2, 's_margin': 0.1, 'n_ensembles': 1, 'proxy_model': None, 'optimizer': 'SGD', 'n_epoch': 30, 'schedule': [80, 120], 'momentum': 0.9, 'lr': 0.1, 'gammas': [0.1, 0.1], 'save_model': False, 'load_ckpt': False, 'add_imagenet': False}\n",
            "Random Seed: 1\n",
            "python version : 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]\n",
            "torch  version : 2.0.0+cu118\n",
            "cudnn  version : 8700\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[init=10000] [query=500] [end=10000]\n",
            "Strategy KMeansSampling successfully loaded...\n",
            "Let's use 1 GPUs!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "[Batch=000] [Loss=4.72]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "==>>[2023-04-30 16:51:24] [Epoch=000/030] [KMeansSampling Need: 00:00:00] [LR=0.1000] [Best : Test Accuracy=0.00, Error=1.00]\n",
            "[Batch=000] [Loss=4.46]\n",
            "\n",
            "==>>[2023-04-30 16:51:33] [Epoch=001/030] [KMeansSampling Need: 00:05:29] [LR=0.1000] [Best : Test Accuracy=0.01, Error=0.99]\n",
            "[Batch=000] [Loss=4.39]\n",
            "\n",
            "==>>[2023-04-30 16:51:41] [Epoch=002/030] [KMeansSampling Need: 00:04:51] [LR=0.1000] [Best : Test Accuracy=0.02, Error=0.98]\n",
            "[Batch=000] [Loss=4.24]\n",
            "\n",
            "==>>[2023-04-30 16:51:51] [Epoch=003/030] [KMeansSampling Need: 00:04:19] [LR=0.1000] [Best : Test Accuracy=0.02, Error=0.98]\n",
            "[Batch=000] [Loss=4.08]\n",
            "\n",
            "==>>[2023-04-30 16:52:01] [Epoch=004/030] [KMeansSampling Need: 00:04:09] [LR=0.1000] [Best : Test Accuracy=0.04, Error=0.96]\n",
            "[Batch=000] [Loss=3.94]\n",
            "\n",
            "==>>[2023-04-30 16:52:09] [Epoch=005/030] [KMeansSampling Need: 00:04:00] [LR=0.1000] [Best : Test Accuracy=0.04, Error=0.96]\n",
            "[Batch=000] [Loss=3.82]\n",
            "\n",
            "==>>[2023-04-30 16:52:18] [Epoch=006/030] [KMeansSampling Need: 00:03:45] [LR=0.1000] [Best : Test Accuracy=0.06, Error=0.94]\n",
            "[Batch=000] [Loss=3.73]\n",
            "\n",
            "==>>[2023-04-30 16:52:28] [Epoch=007/030] [KMeansSampling Need: 00:03:35] [LR=0.1000] [Best : Test Accuracy=0.07, Error=0.93]\n",
            "[Batch=000] [Loss=3.71]\n",
            "\n",
            "==>>[2023-04-30 16:52:36] [Epoch=008/030] [KMeansSampling Need: 00:03:27] [LR=0.1000] [Best : Test Accuracy=0.07, Error=0.93]\n",
            "[Batch=000] [Loss=3.63]\n",
            "\n",
            "==>>[2023-04-30 16:52:45] [Epoch=009/030] [KMeansSampling Need: 00:03:15] [LR=0.1000] [Best : Test Accuracy=0.10, Error=0.90]\n",
            "[Batch=000] [Loss=3.55]\n",
            "\n",
            "==>>[2023-04-30 16:52:54] [Epoch=010/030] [KMeansSampling Need: 00:03:05] [LR=0.1000] [Best : Test Accuracy=0.10, Error=0.90]\n",
            "[Batch=000] [Loss=3.45]\n",
            "\n",
            "==>>[2023-04-30 16:53:03] [Epoch=011/030] [KMeansSampling Need: 00:02:56] [LR=0.1000] [Best : Test Accuracy=0.10, Error=0.90]\n",
            "[Batch=000] [Loss=3.38]\n",
            "\n",
            "==>>[2023-04-30 16:53:12] [Epoch=012/030] [KMeansSampling Need: 00:02:45] [LR=0.1000] [Best : Test Accuracy=0.13, Error=0.87]\n",
            "[Batch=000] [Loss=3.25]\n",
            "\n",
            "==>>[2023-04-30 16:53:21] [Epoch=013/030] [KMeansSampling Need: 00:02:36] [LR=0.1000] [Best : Test Accuracy=0.14, Error=0.86]\n",
            "[Batch=000] [Loss=3.29]\n",
            "\n",
            "==>>[2023-04-30 16:53:30] [Epoch=014/030] [KMeansSampling Need: 00:02:26] [LR=0.1000] [Best : Test Accuracy=0.15, Error=0.85]\n",
            "[Batch=000] [Loss=3.19]\n",
            "\n",
            "==>>[2023-04-30 16:53:39] [Epoch=015/030] [KMeansSampling Need: 00:02:17] [LR=0.1000] [Best : Test Accuracy=0.16, Error=0.84]\n",
            "[Batch=000] [Loss=3.16]\n",
            "\n",
            "==>>[2023-04-30 16:53:47] [Epoch=016/030] [KMeansSampling Need: 00:02:07] [LR=0.1000] [Best : Test Accuracy=0.17, Error=0.83]\n",
            "[Batch=000] [Loss=3.04]\n",
            "\n",
            "==>>[2023-04-30 16:53:56] [Epoch=017/030] [KMeansSampling Need: 00:01:57] [LR=0.1000] [Best : Test Accuracy=0.17, Error=0.83]\n",
            "[Batch=000] [Loss=2.98]\n",
            "\n",
            "==>>[2023-04-30 16:54:05] [Epoch=018/030] [KMeansSampling Need: 00:01:49] [LR=0.1000] [Best : Test Accuracy=0.17, Error=0.83]\n",
            "[Batch=000] [Loss=2.94]\n",
            "\n",
            "==>>[2023-04-30 16:54:13] [Epoch=019/030] [KMeansSampling Need: 00:01:40] [LR=0.1000] [Best : Test Accuracy=0.17, Error=0.83]\n",
            "[Batch=000] [Loss=2.93]\n",
            "\n",
            "==>>[2023-04-30 16:54:23] [Epoch=020/030] [KMeansSampling Need: 00:01:30] [LR=0.1000] [Best : Test Accuracy=0.18, Error=0.82]\n",
            "[Batch=000] [Loss=2.87]\n",
            "\n",
            "==>>[2023-04-30 16:54:32] [Epoch=021/030] [KMeansSampling Need: 00:01:21] [LR=0.1000] [Best : Test Accuracy=0.18, Error=0.82]\n",
            "[Batch=000] [Loss=2.75]\n",
            "\n",
            "==>>[2023-04-30 16:54:41] [Epoch=022/030] [KMeansSampling Need: 00:01:12] [LR=0.1000] [Best : Test Accuracy=0.19, Error=0.81]\n",
            "[Batch=000] [Loss=2.72]\n",
            "\n",
            "==>>[2023-04-30 16:54:50] [Epoch=023/030] [KMeansSampling Need: 00:01:03] [LR=0.1000] [Best : Test Accuracy=0.20, Error=0.80]\n",
            "[Batch=000] [Loss=2.64]\n",
            "\n",
            "==>>[2023-04-30 16:55:00] [Epoch=024/030] [KMeansSampling Need: 00:00:54] [LR=0.1000] [Best : Test Accuracy=0.20, Error=0.80]\n",
            "[Batch=000] [Loss=2.62]\n",
            "\n",
            "==>>[2023-04-30 16:55:08] [Epoch=025/030] [KMeansSampling Need: 00:00:45] [LR=0.1000] [Best : Test Accuracy=0.21, Error=0.79]\n",
            "[Batch=000] [Loss=2.58]\n",
            "\n",
            "==>>[2023-04-30 16:55:17] [Epoch=026/030] [KMeansSampling Need: 00:00:36] [LR=0.1000] [Best : Test Accuracy=0.21, Error=0.79]\n",
            "[Batch=000] [Loss=2.58]\n",
            "\n",
            "==>>[2023-04-30 16:55:27] [Epoch=027/030] [KMeansSampling Need: 00:00:27] [LR=0.1000] [Best : Test Accuracy=0.23, Error=0.77]\n",
            "[Batch=000] [Loss=2.47]\n",
            "\n",
            "==>>[2023-04-30 16:55:35] [Epoch=028/030] [KMeansSampling Need: 00:00:18] [LR=0.1000] [Best : Test Accuracy=0.23, Error=0.77]\n",
            "[Batch=000] [Loss=2.41]\n",
            "\n",
            "==>>[2023-04-30 16:55:44] [Epoch=029/030] [KMeansSampling Need: 00:00:09] [LR=0.1000] [Best : Test Accuracy=0.25, Error=0.75]\n",
            "---- save figure the accuracy/loss curve of train/val into ./save/cifar100\n",
            "==>> Testing accuracy 0.25\n",
            "success!\n"
          ]
        }
      ],
      "source": [
        "!python /content/deep-active-learning/main.py --model ResNet18  --nStart 20  --dataset cifar100 --strategy KMeansSampling --nEnd=20 --n_epoch=30 --seed 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SorXIeny_VNs",
        "outputId": "88791b40-6a7e-4e12-cbfe-deaf75d8bf45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "save path : ./save\n",
            "{'strategy': 'LeastConfidence', 'nQuery': 1, 'nStart': 20.0, 'nEnd': 20.0, 'nEmb': 256, 'seed': 1, 'model': 'ResNet18', 'dataset': 'cifar10', 'data_path': './datasets', 'save_path': './save', 'save_file': 'result.csv', 'hidden_units': 128, 'dropout_rate': 0.3, 'lambda_loss': 1.2, 's_margin': 0.1, 'n_ensembles': 1, 'proxy_model': None, 'optimizer': 'SGD', 'n_epoch': 30, 'schedule': [80, 120], 'momentum': 0.9, 'lr': 0.1, 'gammas': [0.1, 0.1], 'save_model': False, 'load_ckpt': False, 'add_imagenet': False}\n",
            "Random Seed: 1\n",
            "python version : 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]\n",
            "torch  version : 2.0.0+cu118\n",
            "cudnn  version : 8700\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[init=10000] [query=500] [end=10000]\n",
            "Strategy LeastConfidence successfully loaded...\n",
            "Let's use 1 GPUs!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "[Batch=000] [Loss=2.36]\n",
            "[Batch=010] [Loss=2.14]\n",
            "[Batch=020] [Loss=1.90]\n",
            "[Batch=030] [Loss=1.91]\n",
            "\n",
            "==>>[2023-04-30 16:56:08] [Epoch=000/030] [LeastConfidence Need: 00:00:00] [LR=0.1000] [Best : Test Accuracy=0.00, Error=1.00]\n",
            "[Batch=000] [Loss=2.02]\n",
            "[Batch=010] [Loss=1.75]\n",
            "[Batch=020] [Loss=1.68]\n",
            "[Batch=030] [Loss=1.63]\n",
            "\n",
            "==>>[2023-04-30 16:56:16] [Epoch=001/030] [LeastConfidence Need: 00:05:17] [LR=0.1000] [Best : Test Accuracy=0.25, Error=0.75]\n",
            "[Batch=000] [Loss=1.75]\n",
            "[Batch=010] [Loss=1.78]\n",
            "[Batch=020] [Loss=1.63]\n",
            "[Batch=030] [Loss=1.57]\n",
            "\n",
            "==>>[2023-04-30 16:56:25] [Epoch=002/030] [LeastConfidence Need: 00:04:30] [LR=0.1000] [Best : Test Accuracy=0.34, Error=0.66]\n",
            "[Batch=000] [Loss=1.59]\n",
            "[Batch=010] [Loss=1.54]\n",
            "[Batch=020] [Loss=1.50]\n",
            "[Batch=030] [Loss=1.55]\n",
            "\n",
            "==>>[2023-04-30 16:56:35] [Epoch=003/030] [LeastConfidence Need: 00:04:17] [LR=0.1000] [Best : Test Accuracy=0.41, Error=0.59]\n",
            "[Batch=000] [Loss=1.55]\n",
            "[Batch=010] [Loss=1.48]\n",
            "[Batch=020] [Loss=1.54]\n",
            "[Batch=030] [Loss=1.43]\n",
            "\n",
            "==>>[2023-04-30 16:56:44] [Epoch=004/030] [LeastConfidence Need: 00:04:10] [LR=0.1000] [Best : Test Accuracy=0.44, Error=0.56]\n",
            "[Batch=000] [Loss=1.56]\n",
            "[Batch=010] [Loss=1.51]\n",
            "[Batch=020] [Loss=1.39]\n",
            "[Batch=030] [Loss=1.36]\n",
            "\n",
            "==>>[2023-04-30 16:56:54] [Epoch=005/030] [LeastConfidence Need: 00:03:57] [LR=0.1000] [Best : Test Accuracy=0.44, Error=0.56]\n",
            "[Batch=000] [Loss=1.38]\n",
            "[Batch=010] [Loss=1.33]\n",
            "[Batch=020] [Loss=1.32]\n",
            "[Batch=030] [Loss=1.28]\n",
            "\n",
            "==>>[2023-04-30 16:57:04] [Epoch=006/030] [LeastConfidence Need: 00:03:47] [LR=0.1000] [Best : Test Accuracy=0.49, Error=0.51]\n",
            "[Batch=000] [Loss=1.32]\n",
            "[Batch=010] [Loss=1.26]\n",
            "[Batch=020] [Loss=1.24]\n",
            "[Batch=030] [Loss=1.26]\n",
            "\n",
            "==>>[2023-04-30 16:57:13] [Epoch=007/030] [LeastConfidence Need: 00:03:39] [LR=0.1000] [Best : Test Accuracy=0.50, Error=0.50]\n",
            "[Batch=000] [Loss=1.22]\n",
            "[Batch=010] [Loss=1.27]\n",
            "[Batch=020] [Loss=1.16]\n",
            "[Batch=030] [Loss=1.16]\n",
            "\n",
            "==>>[2023-04-30 16:57:23] [Epoch=008/030] [LeastConfidence Need: 00:03:30] [LR=0.1000] [Best : Test Accuracy=0.52, Error=0.48]\n",
            "[Batch=000] [Loss=1.16]\n",
            "[Batch=010] [Loss=1.10]\n",
            "[Batch=020] [Loss=1.07]\n",
            "[Batch=030] [Loss=1.27]\n",
            "\n",
            "==>>[2023-04-30 16:57:33] [Epoch=009/030] [LeastConfidence Need: 00:03:20] [LR=0.1000] [Best : Test Accuracy=0.54, Error=0.46]\n",
            "[Batch=000] [Loss=1.18]\n",
            "[Batch=010] [Loss=1.15]\n",
            "[Batch=020] [Loss=1.16]\n",
            "[Batch=030] [Loss=1.02]\n",
            "\n",
            "==>>[2023-04-30 16:57:43] [Epoch=010/030] [LeastConfidence Need: 00:03:12] [LR=0.1000] [Best : Test Accuracy=0.55, Error=0.45]\n",
            "[Batch=000] [Loss=0.97]\n",
            "[Batch=010] [Loss=1.03]\n",
            "[Batch=020] [Loss=1.10]\n",
            "[Batch=030] [Loss=1.00]\n",
            "\n",
            "==>>[2023-04-30 16:57:52] [Epoch=011/030] [LeastConfidence Need: 00:03:04] [LR=0.1000] [Best : Test Accuracy=0.56, Error=0.44]\n",
            "[Batch=000] [Loss=1.07]\n",
            "[Batch=010] [Loss=1.03]\n",
            "[Batch=020] [Loss=1.00]\n",
            "[Batch=030] [Loss=1.01]\n",
            "\n",
            "==>>[2023-04-30 16:58:02] [Epoch=012/030] [LeastConfidence Need: 00:02:52] [LR=0.1000] [Best : Test Accuracy=0.58, Error=0.42]\n",
            "[Batch=000] [Loss=0.95]\n",
            "[Batch=010] [Loss=0.93]\n",
            "[Batch=020] [Loss=0.97]\n",
            "[Batch=030] [Loss=0.97]\n",
            "\n",
            "==>>[2023-04-30 16:58:12] [Epoch=013/030] [LeastConfidence Need: 00:02:43] [LR=0.1000] [Best : Test Accuracy=0.58, Error=0.42]\n",
            "[Batch=000] [Loss=0.89]\n",
            "[Batch=010] [Loss=0.98]\n",
            "[Batch=020] [Loss=0.95]\n",
            "[Batch=030] [Loss=0.80]\n",
            "\n",
            "==>>[2023-04-30 16:58:20] [Epoch=014/030] [LeastConfidence Need: 00:02:34] [LR=0.1000] [Best : Test Accuracy=0.59, Error=0.41]\n",
            "[Batch=000] [Loss=0.96]\n",
            "[Batch=010] [Loss=0.98]\n",
            "[Batch=020] [Loss=0.96]\n",
            "[Batch=030] [Loss=0.82]\n",
            "\n",
            "==>>[2023-04-30 16:58:30] [Epoch=015/030] [LeastConfidence Need: 00:02:23] [LR=0.1000] [Best : Test Accuracy=0.59, Error=0.41]\n",
            "[Batch=000] [Loss=0.91]\n",
            "[Batch=010] [Loss=0.93]\n",
            "[Batch=020] [Loss=0.81]\n",
            "[Batch=030] [Loss=0.92]\n",
            "\n",
            "==>>[2023-04-30 16:58:40] [Epoch=016/030] [LeastConfidence Need: 00:02:14] [LR=0.1000] [Best : Test Accuracy=0.64, Error=0.36]\n",
            "[Batch=000] [Loss=0.91]\n",
            "[Batch=010] [Loss=0.77]\n",
            "[Batch=020] [Loss=0.83]\n",
            "[Batch=030] [Loss=0.88]\n",
            "\n",
            "==>>[2023-04-30 16:58:49] [Epoch=017/030] [LeastConfidence Need: 00:02:05] [LR=0.1000] [Best : Test Accuracy=0.64, Error=0.36]\n",
            "[Batch=000] [Loss=1.02]\n",
            "[Batch=010] [Loss=0.92]\n",
            "[Batch=020] [Loss=0.78]\n",
            "[Batch=030] [Loss=0.82]\n",
            "\n",
            "==>>[2023-04-30 16:58:59] [Epoch=018/030] [LeastConfidence Need: 00:01:54] [LR=0.1000] [Best : Test Accuracy=0.64, Error=0.36]\n",
            "[Batch=000] [Loss=0.74]\n",
            "[Batch=010] [Loss=0.98]\n",
            "[Batch=020] [Loss=0.83]\n",
            "[Batch=030] [Loss=0.75]\n",
            "\n",
            "==>>[2023-04-30 16:59:09] [Epoch=019/030] [LeastConfidence Need: 00:01:45] [LR=0.1000] [Best : Test Accuracy=0.68, Error=0.32]\n",
            "[Batch=000] [Loss=0.76]\n",
            "[Batch=010] [Loss=0.91]\n",
            "[Batch=020] [Loss=0.83]\n",
            "[Batch=030] [Loss=0.70]\n",
            "\n",
            "==>>[2023-04-30 16:59:17] [Epoch=020/030] [LeastConfidence Need: 00:01:36] [LR=0.1000] [Best : Test Accuracy=0.68, Error=0.32]\n",
            "[Batch=000] [Loss=0.69]\n",
            "[Batch=010] [Loss=0.82]\n",
            "[Batch=020] [Loss=0.82]\n",
            "[Batch=030] [Loss=0.71]\n",
            "\n",
            "==>>[2023-04-30 16:59:27] [Epoch=021/030] [LeastConfidence Need: 00:01:25] [LR=0.1000] [Best : Test Accuracy=0.68, Error=0.32]\n",
            "[Batch=000] [Loss=0.70]\n",
            "[Batch=010] [Loss=0.81]\n",
            "[Batch=020] [Loss=0.66]\n",
            "[Batch=030] [Loss=0.81]\n",
            "\n",
            "==>>[2023-04-30 16:59:37] [Epoch=022/030] [LeastConfidence Need: 00:01:16] [LR=0.1000] [Best : Test Accuracy=0.68, Error=0.32]\n",
            "[Batch=000] [Loss=0.69]\n",
            "[Batch=010] [Loss=0.71]\n",
            "[Batch=020] [Loss=0.70]\n",
            "[Batch=030] [Loss=0.68]\n",
            "\n",
            "==>>[2023-04-30 16:59:46] [Epoch=023/030] [LeastConfidence Need: 00:01:07] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.71]\n",
            "[Batch=010] [Loss=0.73]\n",
            "[Batch=020] [Loss=0.98]\n",
            "[Batch=030] [Loss=0.64]\n",
            "\n",
            "==>>[2023-04-30 16:59:56] [Epoch=024/030] [LeastConfidence Need: 00:00:57] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.77]\n",
            "[Batch=010] [Loss=0.80]\n",
            "[Batch=020] [Loss=0.76]\n",
            "[Batch=030] [Loss=0.64]\n",
            "\n",
            "==>>[2023-04-30 17:00:06] [Epoch=025/030] [LeastConfidence Need: 00:00:47] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.72]\n",
            "[Batch=010] [Loss=0.72]\n",
            "[Batch=020] [Loss=0.65]\n",
            "[Batch=030] [Loss=0.62]\n",
            "\n",
            "==>>[2023-04-30 17:00:14] [Epoch=026/030] [LeastConfidence Need: 00:00:38] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.68]\n",
            "[Batch=010] [Loss=0.60]\n",
            "[Batch=020] [Loss=0.55]\n",
            "[Batch=030] [Loss=0.55]\n",
            "\n",
            "==>>[2023-04-30 17:00:25] [Epoch=027/030] [LeastConfidence Need: 00:00:28] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.65]\n",
            "[Batch=010] [Loss=0.79]\n",
            "[Batch=020] [Loss=0.81]\n",
            "[Batch=030] [Loss=0.76]\n",
            "\n",
            "==>>[2023-04-30 17:00:35] [Epoch=028/030] [LeastConfidence Need: 00:00:19] [LR=0.1000] [Best : Test Accuracy=0.69, Error=0.31]\n",
            "[Batch=000] [Loss=0.53]\n",
            "[Batch=010] [Loss=0.54]\n",
            "[Batch=020] [Loss=0.60]\n",
            "[Batch=030] [Loss=0.61]\n",
            "\n",
            "==>>[2023-04-30 17:00:44] [Epoch=029/030] [LeastConfidence Need: 00:00:09] [LR=0.1000] [Best : Test Accuracy=0.71, Error=0.29]\n",
            "---- save figure the accuracy/loss curve of train/val into ./save/cifar10\n",
            "==>> Testing accuracy 0.6947\n",
            "success!\n"
          ]
        }
      ],
      "source": [
        "!python /content/deep-active-learning/main.py --model ResNet18  --nStart 20  --dataset cifar100 --strategy LeastConfidence --nEnd=20 --n_epoch=30 --seed 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N6o1irO_VNs",
        "outputId": "7a808d0c-81b2-403d-f357-5a210ba62f67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "save path : ./save\n",
            "{'strategy': 'MarginSampling', 'nQuery': 1, 'nStart': 20.0, 'nEnd': 20.0, 'nEmb': 256, 'seed': 1, 'model': 'ResNet18', 'dataset': 'cifar100', 'data_path': './datasets', 'save_path': './save', 'save_file': 'result.csv', 'hidden_units': 128, 'dropout_rate': 0.3, 'lambda_loss': 1.2, 's_margin': 0.1, 'n_ensembles': 1, 'proxy_model': None, 'optimizer': 'SGD', 'n_epoch': 30, 'schedule': [80, 120], 'momentum': 0.9, 'lr': 0.1, 'gammas': [0.1, 0.1], 'save_model': False, 'load_ckpt': False, 'add_imagenet': False}\n",
            "Random Seed: 1\n",
            "python version : 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]\n",
            "torch  version : 2.0.0+cu118\n",
            "cudnn  version : 8700\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[init=10000] [query=500] [end=10000]\n",
            "Strategy MarginSampling successfully loaded...\n",
            "Let's use 1 GPUs!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "[Batch=000] [Loss=4.72]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "==>>[2023-04-30 17:02:03] [Epoch=000/030] [MarginSampling Need: 00:00:00] [LR=0.1000] [Best : Test Accuracy=0.00, Error=1.00]\n",
            "[Batch=000] [Loss=4.46]\n",
            "\n",
            "==>>[2023-04-30 17:02:12] [Epoch=001/030] [MarginSampling Need: 00:05:43] [LR=0.1000] [Best : Test Accuracy=0.01, Error=0.99]\n",
            "[Batch=000] [Loss=4.39]\n",
            "\n",
            "==>>[2023-04-30 17:02:21] [Epoch=002/030] [MarginSampling Need: 00:04:54] [LR=0.1000] [Best : Test Accuracy=0.02, Error=0.98]\n",
            "[Batch=000] [Loss=4.24]\n",
            "\n",
            "==>>[2023-04-30 17:02:30] [Epoch=003/030] [MarginSampling Need: 00:04:27] [LR=0.1000] [Best : Test Accuracy=0.02, Error=0.98]\n",
            "[Batch=000] [Loss=4.09]\n",
            "\n",
            "==>>[2023-04-30 17:02:39] [Epoch=004/030] [MarginSampling Need: 00:04:12] [LR=0.1000] [Best : Test Accuracy=0.04, Error=0.96]\n",
            "[Batch=000] [Loss=3.95]\n",
            "\n",
            "==>>[2023-04-30 17:02:48] [Epoch=005/030] [MarginSampling Need: 00:04:00] [LR=0.1000] [Best : Test Accuracy=0.04, Error=0.96]\n",
            "[Batch=000] [Loss=3.82]\n",
            "\n",
            "==>>[2023-04-30 17:02:57] [Epoch=006/030] [MarginSampling Need: 00:03:46] [LR=0.1000] [Best : Test Accuracy=0.05, Error=0.95]\n",
            "[Batch=000] [Loss=3.72]\n",
            "\n",
            "==>>[2023-04-30 17:03:06] [Epoch=007/030] [MarginSampling Need: 00:03:36] [LR=0.1000] [Best : Test Accuracy=0.07, Error=0.93]\n",
            "[Batch=000] [Loss=3.65]\n",
            "\n",
            "==>>[2023-04-30 17:03:14] [Epoch=008/030] [MarginSampling Need: 00:03:25] [LR=0.1000] [Best : Test Accuracy=0.09, Error=0.91]\n",
            "[Batch=000] [Loss=3.61]\n",
            "\n",
            "==>>[2023-04-30 17:03:24] [Epoch=009/030] [MarginSampling Need: 00:03:14] [LR=0.1000] [Best : Test Accuracy=0.10, Error=0.90]\n",
            "[Batch=000] [Loss=3.55]\n",
            "\n",
            "==>>[2023-04-30 17:03:33] [Epoch=010/030] [MarginSampling Need: 00:03:05] [LR=0.1000] [Best : Test Accuracy=0.10, Error=0.90]\n",
            "[Batch=000] [Loss=3.49]\n",
            "\n",
            "==>>[2023-04-30 17:03:41] [Epoch=011/030] [MarginSampling Need: 00:02:55] [LR=0.1000] [Best : Test Accuracy=0.12, Error=0.88]\n",
            "[Batch=000] [Loss=3.35]\n",
            "\n",
            "==>>[2023-04-30 17:03:50] [Epoch=012/030] [MarginSampling Need: 00:02:45] [LR=0.1000] [Best : Test Accuracy=0.13, Error=0.87]\n",
            "[Batch=000] [Loss=3.26]\n",
            "\n",
            "==>>[2023-04-30 17:04:00] [Epoch=013/030] [MarginSampling Need: 00:02:36] [LR=0.1000] [Best : Test Accuracy=0.13, Error=0.87]\n",
            "[Batch=000] [Loss=3.29]\n",
            "\n",
            "==>>[2023-04-30 17:04:08] [Epoch=014/030] [MarginSampling Need: 00:02:26] [LR=0.1000] [Best : Test Accuracy=0.13, Error=0.87]\n",
            "[Batch=000] [Loss=3.18]\n",
            "\n",
            "==>>[2023-04-30 17:04:18] [Epoch=015/030] [MarginSampling Need: 00:02:17] [LR=0.1000] [Best : Test Accuracy=0.14, Error=0.86]\n",
            "[Batch=000] [Loss=3.13]\n",
            "\n",
            "==>>[2023-04-30 17:04:27] [Epoch=016/030] [MarginSampling Need: 00:02:08] [LR=0.1000] [Best : Test Accuracy=0.17, Error=0.83]\n",
            "[Batch=000] [Loss=3.07]\n",
            "\n",
            "==>>[2023-04-30 17:04:35] [Epoch=017/030] [MarginSampling Need: 00:01:59] [LR=0.1000] [Best : Test Accuracy=0.17, Error=0.83]\n",
            "[Batch=000] [Loss=2.99]\n",
            "\n",
            "==>>[2023-04-30 17:04:45] [Epoch=018/030] [MarginSampling Need: 00:01:49] [LR=0.1000] [Best : Test Accuracy=0.17, Error=0.83]\n",
            "[Batch=000] [Loss=2.92]\n",
            "\n",
            "==>>[2023-04-30 17:04:54] [Epoch=019/030] [MarginSampling Need: 00:01:40] [LR=0.1000] [Best : Test Accuracy=0.18, Error=0.82]\n",
            "[Batch=000] [Loss=2.91]\n",
            "\n",
            "==>>[2023-04-30 17:05:02] [Epoch=020/030] [MarginSampling Need: 00:01:31] [LR=0.1000] [Best : Test Accuracy=0.18, Error=0.82]\n",
            "[Batch=000] [Loss=2.86]\n",
            "\n",
            "==>>[2023-04-30 17:05:12] [Epoch=021/030] [MarginSampling Need: 00:01:21] [LR=0.1000] [Best : Test Accuracy=0.18, Error=0.82]\n",
            "[Batch=000] [Loss=2.75]\n",
            "\n",
            "==>>[2023-04-30 17:05:21] [Epoch=022/030] [MarginSampling Need: 00:01:12] [LR=0.1000] [Best : Test Accuracy=0.20, Error=0.80]\n",
            "[Batch=000] [Loss=2.73]\n",
            "\n",
            "==>>[2023-04-30 17:05:29] [Epoch=023/030] [MarginSampling Need: 00:01:03] [LR=0.1000] [Best : Test Accuracy=0.20, Error=0.80]\n",
            "[Batch=000] [Loss=2.68]\n",
            "\n",
            "==>>[2023-04-30 17:05:38] [Epoch=024/030] [MarginSampling Need: 00:00:54] [LR=0.1000] [Best : Test Accuracy=0.21, Error=0.79]\n",
            "[Batch=000] [Loss=2.61]\n",
            "\n",
            "==>>[2023-04-30 17:05:47] [Epoch=025/030] [MarginSampling Need: 00:00:45] [LR=0.1000] [Best : Test Accuracy=0.21, Error=0.79]\n",
            "[Batch=000] [Loss=2.57]\n",
            "\n",
            "==>>[2023-04-30 17:05:56] [Epoch=026/030] [MarginSampling Need: 00:00:36] [LR=0.1000] [Best : Test Accuracy=0.21, Error=0.79]\n",
            "[Batch=000] [Loss=2.56]\n",
            "\n",
            "==>>[2023-04-30 17:06:05] [Epoch=027/030] [MarginSampling Need: 00:00:27] [LR=0.1000] [Best : Test Accuracy=0.23, Error=0.77]\n",
            "[Batch=000] [Loss=2.40]\n",
            "\n",
            "==>>[2023-04-30 17:06:14] [Epoch=028/030] [MarginSampling Need: 00:00:18] [LR=0.1000] [Best : Test Accuracy=0.24, Error=0.76]\n",
            "[Batch=000] [Loss=2.39]\n",
            "\n",
            "==>>[2023-04-30 17:06:23] [Epoch=029/030] [MarginSampling Need: 00:00:09] [LR=0.1000] [Best : Test Accuracy=0.24, Error=0.76]\n",
            "---- save figure the accuracy/loss curve of train/val into ./save/cifar100\n",
            "==>> Testing accuracy 0.2323\n",
            "success!\n"
          ]
        }
      ],
      "source": [
        "!python /content/deep-active-learning/main.py --model ResNet18  --nStart 20  --dataset cifar100 --strategy MarginSampling --nEnd=20 --n_epoch=30 --seed 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBqcV6_x_VNt",
        "outputId": "e6f236b4-748f-4e23-c125-68906ba37b0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "save path : ./save\n",
            "{'strategy': 'RandomSampling', 'nQuery': 1, 'nStart': 20.0, 'nEnd': 20.0, 'nEmb': 256, 'seed': 1, 'model': 'ResNet18', 'dataset': 'cifar100', 'data_path': './datasets', 'save_path': './save', 'save_file': 'result.csv', 'hidden_units': 128, 'dropout_rate': 0.3, 'lambda_loss': 1.2, 's_margin': 0.1, 'n_ensembles': 1, 'proxy_model': None, 'optimizer': 'SGD', 'n_epoch': 30, 'schedule': [80, 120], 'momentum': 0.9, 'lr': 0.1, 'gammas': [0.1, 0.1], 'save_model': False, 'load_ckpt': False, 'add_imagenet': False}\n",
            "Random Seed: 1\n",
            "python version : 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]\n",
            "torch  version : 2.0.0+cu118\n",
            "cudnn  version : 8700\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[init=10000] [query=500] [end=10000]\n",
            "Strategy RandomSampling successfully loaded...\n",
            "Let's use 1 GPUs!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "[Batch=000] [Loss=4.72]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "==>>[2023-04-30 17:13:01] [Epoch=000/030] [RandomSampling Need: 00:00:00] [LR=0.1000] [Best : Test Accuracy=0.00, Error=1.00]\n",
            "[Batch=000] [Loss=4.46]\n",
            "\n",
            "==>>[2023-04-30 17:13:10] [Epoch=001/030] [RandomSampling Need: 00:05:13] [LR=0.1000] [Best : Test Accuracy=0.01, Error=0.99]\n",
            "[Batch=000] [Loss=4.40]\n",
            "\n",
            "==>>[2023-04-30 17:13:19] [Epoch=002/030] [RandomSampling Need: 00:04:41] [LR=0.1000] [Best : Test Accuracy=0.02, Error=0.98]\n",
            "[Batch=000] [Loss=4.25]\n",
            "\n",
            "==>>[2023-04-30 17:13:28] [Epoch=003/030] [RandomSampling Need: 00:04:20] [LR=0.1000] [Best : Test Accuracy=0.02, Error=0.98]\n",
            "[Batch=000] [Loss=4.09]\n",
            "\n",
            "==>>[2023-04-30 17:13:37] [Epoch=004/030] [RandomSampling Need: 00:04:07] [LR=0.1000] [Best : Test Accuracy=0.03, Error=0.97]\n",
            "[Batch=000] [Loss=4.00]\n",
            "\n",
            "==>>[2023-04-30 17:13:46] [Epoch=005/030] [RandomSampling Need: 00:03:54] [LR=0.1000] [Best : Test Accuracy=0.05, Error=0.95]\n",
            "[Batch=000] [Loss=3.87]\n",
            "\n",
            "==>>[2023-04-30 17:13:55] [Epoch=006/030] [RandomSampling Need: 00:03:42] [LR=0.1000] [Best : Test Accuracy=0.05, Error=0.95]\n",
            "[Batch=000] [Loss=3.79]\n",
            "\n",
            "==>>[2023-04-30 17:14:04] [Epoch=007/030] [RandomSampling Need: 00:03:33] [LR=0.1000] [Best : Test Accuracy=0.06, Error=0.94]\n",
            "[Batch=000] [Loss=3.70]\n",
            "\n",
            "==>>[2023-04-30 17:14:13] [Epoch=008/030] [RandomSampling Need: 00:03:22] [LR=0.1000] [Best : Test Accuracy=0.07, Error=0.93]\n",
            "[Batch=000] [Loss=3.62]\n",
            "\n",
            "==>>[2023-04-30 17:14:22] [Epoch=009/030] [RandomSampling Need: 00:03:13] [LR=0.1000] [Best : Test Accuracy=0.07, Error=0.93]\n",
            "[Batch=000] [Loss=3.61]\n",
            "\n",
            "==>>[2023-04-30 17:14:31] [Epoch=010/030] [RandomSampling Need: 00:03:03] [LR=0.1000] [Best : Test Accuracy=0.10, Error=0.90]\n",
            "[Batch=000] [Loss=3.50]\n",
            "\n",
            "==>>[2023-04-30 17:14:40] [Epoch=011/030] [RandomSampling Need: 00:02:53] [LR=0.1000] [Best : Test Accuracy=0.12, Error=0.88]\n",
            "[Batch=000] [Loss=3.40]\n",
            "\n",
            "==>>[2023-04-30 17:14:49] [Epoch=012/030] [RandomSampling Need: 00:02:44] [LR=0.1000] [Best : Test Accuracy=0.14, Error=0.86]\n",
            "[Batch=000] [Loss=3.31]\n",
            "\n",
            "==>>[2023-04-30 17:14:57] [Epoch=013/030] [RandomSampling Need: 00:02:35] [LR=0.1000] [Best : Test Accuracy=0.14, Error=0.86]\n",
            "[Batch=000] [Loss=3.35]\n",
            "\n",
            "==>>[2023-04-30 17:15:07] [Epoch=014/030] [RandomSampling Need: 00:02:25] [LR=0.1000] [Best : Test Accuracy=0.15, Error=0.85]\n",
            "[Batch=000] [Loss=3.26]\n",
            "\n",
            "==>>[2023-04-30 17:15:16] [Epoch=015/030] [RandomSampling Need: 00:02:16] [LR=0.1000] [Best : Test Accuracy=0.16, Error=0.84]\n",
            "[Batch=000] [Loss=3.21]\n",
            "\n",
            "==>>[2023-04-30 17:15:25] [Epoch=016/030] [RandomSampling Need: 00:02:07] [LR=0.1000] [Best : Test Accuracy=0.17, Error=0.83]\n",
            "[Batch=000] [Loss=3.12]\n",
            "\n",
            "==>>[2023-04-30 17:15:34] [Epoch=017/030] [RandomSampling Need: 00:01:58] [LR=0.1000] [Best : Test Accuracy=0.17, Error=0.83]\n",
            "[Batch=000] [Loss=3.10]\n",
            "\n",
            "==>>[2023-04-30 17:15:43] [Epoch=018/030] [RandomSampling Need: 00:01:49] [LR=0.1000] [Best : Test Accuracy=0.18, Error=0.82]\n",
            "[Batch=000] [Loss=2.98]\n",
            "\n",
            "==>>[2023-04-30 17:15:52] [Epoch=019/030] [RandomSampling Need: 00:01:40] [LR=0.1000] [Best : Test Accuracy=0.18, Error=0.82]\n",
            "[Batch=000] [Loss=2.98]\n",
            "\n",
            "==>>[2023-04-30 17:16:01] [Epoch=020/030] [RandomSampling Need: 00:01:30] [LR=0.1000] [Best : Test Accuracy=0.19, Error=0.81]\n",
            "[Batch=000] [Loss=2.92]\n",
            "\n",
            "==>>[2023-04-30 17:16:10] [Epoch=021/030] [RandomSampling Need: 00:01:21] [LR=0.1000] [Best : Test Accuracy=0.20, Error=0.80]\n",
            "[Batch=000] [Loss=2.78]\n",
            "\n",
            "==>>[2023-04-30 17:16:19] [Epoch=022/030] [RandomSampling Need: 00:01:12] [LR=0.1000] [Best : Test Accuracy=0.20, Error=0.80]\n",
            "[Batch=000] [Loss=2.77]\n",
            "\n",
            "==>>[2023-04-30 17:16:28] [Epoch=023/030] [RandomSampling Need: 00:01:03] [LR=0.1000] [Best : Test Accuracy=0.20, Error=0.80]\n",
            "[Batch=000] [Loss=2.70]\n",
            "\n",
            "==>>[2023-04-30 17:16:38] [Epoch=024/030] [RandomSampling Need: 00:00:54] [LR=0.1000] [Best : Test Accuracy=0.22, Error=0.78]\n",
            "[Batch=000] [Loss=2.71]\n",
            "\n",
            "==>>[2023-04-30 17:16:46] [Epoch=025/030] [RandomSampling Need: 00:00:45] [LR=0.1000] [Best : Test Accuracy=0.22, Error=0.78]\n",
            "[Batch=000] [Loss=2.63]\n",
            "\n",
            "==>>[2023-04-30 17:16:56] [Epoch=026/030] [RandomSampling Need: 00:00:36] [LR=0.1000] [Best : Test Accuracy=0.22, Error=0.78]\n",
            "[Batch=000] [Loss=2.64]\n",
            "\n",
            "==>>[2023-04-30 17:17:05] [Epoch=027/030] [RandomSampling Need: 00:00:27] [LR=0.1000] [Best : Test Accuracy=0.22, Error=0.78]\n",
            "[Batch=000] [Loss=2.50]\n",
            "\n",
            "==>>[2023-04-30 17:17:14] [Epoch=028/030] [RandomSampling Need: 00:00:18] [LR=0.1000] [Best : Test Accuracy=0.22, Error=0.78]\n",
            "[Batch=000] [Loss=2.47]\n",
            "\n",
            "==>>[2023-04-30 17:17:23] [Epoch=029/030] [RandomSampling Need: 00:00:09] [LR=0.1000] [Best : Test Accuracy=0.23, Error=0.77]\n",
            "---- save figure the accuracy/loss curve of train/val into ./save/cifar100\n",
            "==>> Testing accuracy 0.2546\n",
            "success!\n"
          ]
        }
      ],
      "source": [
        "!python /content/deep-active-learning/main.py --model ResNet18  --nStart 20  --dataset cifar100 --strategy RandomSampling --nEnd=20 --n_epoch=30 --seed 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udWEJ82G_cLy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ba4LRIE4A0rb",
        "-X1xU2TI4gi6",
        "76cq-MyU0sUs",
        "RjwdeFCP3CXY"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}